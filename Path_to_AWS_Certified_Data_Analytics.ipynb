{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path to AWS Certified Data Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"src/Domains.png\" width=\"779\" height=\"196\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect (18%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "\ttable.tableizer-table {\n",
    "\t\tfont-size: 12px;\n",
    "\t\tborder: 1px solid #CCC; \n",
    "\t\tfont-family: Arial, Helvetica, sans-serif;\n",
    "\t} \n",
    "\t.tableizer-table td {\n",
    "\t\tpadding: 4px;\n",
    "\t\tmargin: 3px;\n",
    "\t\tborder: 1px solid #CCC;\n",
    "\t}\n",
    "\t.tableizer-table th {\n",
    "\t\tbackground-color: #104E8B; \n",
    "\t\tcolor: #FFF;\n",
    "\t\tfont-weight: bold;\n",
    "\t}\n",
    "</style>\n",
    "<table class=\"tableizer-table\" > \n",
    "           <tbody> \n",
    "            <tr class=\"tableizer-firstrow\"> \n",
    "             <th width=\"15%\">&nbsp;</th> \n",
    "             <th style=\"text-align: center;\">Batch processing</th> \n",
    "             <th style=\"text-align: center;\">Stream processing<br> </th> \n",
    "            </tr> \n",
    "            <tr> \n",
    "             <td width=\"15%\">Data scope</td> \n",
    "             <td style=\"text-align: left;\">Queries or processing over all or most of the data in the dataset.</td> \n",
    "             <td style=\"text-align: left;\">Queries or processing over data within a rolling time window, or on just the most recent data record.</td> \n",
    "            </tr> \n",
    "            <tr> \n",
    "             <td width=\"15%\">Data size</td> \n",
    "             <td style=\"text-align: left;\">Large batches of data.</td> \n",
    "             <td style=\"text-align: left;\">Individual records or micro batches consisting of a few records.</td> \n",
    "            </tr> \n",
    "            <tr> \n",
    "             <td width=\"15%\">Performance</td> \n",
    "             <td style=\"text-align: left;\">Latencies in minutes to hours.</td> \n",
    "             <td style=\"text-align: left;\">Requires latency in the order of seconds or milliseconds.</td> \n",
    "            </tr> \n",
    "            <tr> \n",
    "             <td width=\"15%\">Analyses</td> \n",
    "             <td style=\"text-align: left;\">Complex analytics.</td> \n",
    "             <td style=\"text-align: left;\">Simple response functions, aggregates, and rolling metrics.</td> \n",
    "            </tr> \n",
    "           </tbody> \n",
    "          </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>At most once</h5>\n",
    "At most once means that a message will never be delivered more than one time. However, a message could be lost. Note that, this\n",
    "pattern is only accepted when the data itself is low value or loses value if it is not immediately processed.\n",
    "<h5>At least once</h5>\n",
    "delivery replays recent events starting from an acknowledged (known processed) event.\n",
    "This approach presents some data more than once to the processing pipeline. The typical implementation\n",
    "returns at-least-once delivery checkpoints to a safe point (so you know that they have been processed).\n",
    "<h5>Exactly Once</h5> \n",
    "This type of processing is the ideal because each event is processed exactly once. It avoids the difficult side\n",
    "effects and considerations raised by the other two deliveries. You can achieve\n",
    "exactly-once semantics using idempotency in combination with at-least-once delivery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review operational characteristics of AWS ingestion services\n",
    "\n",
    "<style type=\"text/css\">\n",
    "\ttable.tableizer-table {\n",
    "\t\tfont-size: 12px;\n",
    "\t\tborder: 1px solid #CCC; \n",
    "\t\tfont-family: Arial, Helvetica, sans-serif;\n",
    "\t} \n",
    "\t.tableizer-table td {\n",
    "\t\tpadding: 4px;\n",
    "\t\tmargin: 3px;\n",
    "\t\tborder: 1px solid #CCC;\n",
    "\t}\n",
    "\t.tableizer-table th {\n",
    "\t\tbackground-color: #104E8B; \n",
    "\t\tcolor: #FFF;\n",
    "\t\tfont-weight: bold;\n",
    "\t}\n",
    "</style>\n",
    "<table class=\"tableizer-table\">\n",
    "<thead><tr class=\"tableizer-firstrow\"><th></th><th>Amazon DynamoDB Streams</th><th>Amazon Kinesis Data Stream(KDS)</th><th>Amazon Kinesis Data Firehose</th><th>Amazon MSK</th><th>Apache Kafka(on EC2)</th><th>Amazon SQS Standard</th><th>Amazon SQS FIFO</th></tr></thead><tbody>\n",
    " <tr><td>AWS Managed</td><td>Yes</td><td>Yes</td><td>Yes</td><td>Yes</td><td>No</td><td>Yes</td><td>Yes</td></tr>\n",
    " <tr><td>Use cases</td><td><ul><li>Replication</li><li>Mobile app</li><li>Global multi-player game</li></ul></td><td> real-time data streaming</td><td>Loader streaming data into data lakes, data stores and analytics tools</td><td>real-time streaming data pipelines and applications</td><td>real-time streaming data pipelines and applications</td><td>Queue</td><td>Queue</td></tr>\n",
    " <tr><td>Guaranteed ordering</td><td>Yes</td><td>Yes</td><td>No</td><td>Yes</td><td>Yes</td><td>No</td><td>Yes</td></tr>\n",
    " <tr><td>Delivery(deduping)</td><td>Exactly once</td><td>At least once</td><td>At least once</td><td>At least once</td><td>At least once</td><td>At least once</td><td>Exactly once</td></tr>\n",
    " <tr><td>Data retention period</td><td>24 hours</td><td>7 days</td><td>N/A</td><td>Configurable</td><td>Configurable</td><td>14 days</td><td>14 days</td></tr>\n",
    " <tr><td>Availability</td><td>3 AZ</td><td>3 AZ</td><td>3 AZ</td><td>Configurable Multi AZ</td><td>Configurable Multi AZ</td><td>3 AZ</td><td>3 AZ</td></tr>\n",
    " <tr><td>Scale/throughput</td><td>No limit / Automatic</td><td>No limit / shards</td><td>No limit / Automatic</td><td>up to 30 / brokers</td><td>No limit / nodes</td><td>No limit / Automatic</td><td>300 TPS / queue</td></tr>\n",
    " <tr><td>Parallel Consumption</td><td>Yes</td><td>Yes</td><td>No</td><td>Yes</td><td>Yes</td><td>No</td><td>No</td></tr>\n",
    " <tr><td>Row/Object Size</td><td>400 KB</td><td>1 MB</td><td>Destination row / object size</td><td>Varies</td><td>Varies</td><td>256 KB</td><td>256 KB</td></tr>\n",
    " <tr><td>Stream MapReduce</td><td>Yes</td><td>Yes</td><td>N/A</td><td>Yes</td><td>Yes</td><td>N/A</td><td>N/A</td></tr>\n",
    "<tr><td>Cost</td><td>Higher</td><td>Low</td><td>Low</td><td>Low</td><td>Low(+Admin)</td><td>Low-Medium</td><td>Low-Medium</td></tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon MSK: up to 30 brokers, but, you can request a limit increase in the AWS Support Center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <ol>\n",
    "  <li>How quickly do you need the analytics results?  In real time, in seconds, or is an hour a more appropriate time frame?</li>\n",
    "  <li>Where is the data coming from?</li>\n",
    "</ol> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing ingestion services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "\ttable.tableizer-table {\n",
    "\t\tfont-size: 12px;\n",
    "\t\tborder: 1px solid #CCC; \n",
    "\t\tfont-family: Arial, Helvetica, sans-serif;\n",
    "\t} \n",
    "\t.tableizer-table td {\n",
    "\t\tpadding: 4px;\n",
    "\t\tmargin: 3px;\n",
    "\t\tborder: 1px solid #CCC;\n",
    "\t}\n",
    "\t.tableizer-table th {\n",
    "\t\tbackground-color: #104E8B; \n",
    "\t\tcolor: #FFF;\n",
    "\t\tfont-weight: bold;\n",
    "\t}\n",
    "</style>\n",
    "<table class=\"tableizer-table\">\n",
    "<thead><tr class=\"tableizer-firstrow\"><th></th><th>Kinesis Data Streams</th><th>Kinesis Data Firehose</th><th>AWS DMS</th><th>AWS Glue</th></tr></thead><tbody>\n",
    " <tr><td>scale and throughput</td><td>Each shard can support up to 1,000 PUT records per second. However, you can increase the number of shards limitlessly. One shard provides a capacity of 1 MB/sec data input and 2 MB/sec data output</td><td>Kinesis Data Firehose will automatically scale to match the throughput of your data, without any manual intervention or developer overhead.</td><td>AWS DMS uses Amazon EC2 instances as the replication instance. You can scale up or down your replication instance.</td><td>AWS Glue uses a scale-out Apache Spark environment to load your data into its destination. To scale out, you specify the number of DPUs (data processing units) that you want to allocate to your ETL jobs.</td></tr>\n",
    " <tr><td>fault tolerance</td><td>3 AZ</td><td>3 AZ</td><td>You have the option of enabling Multi-AZ which provides a replication stream that is fault-tolerant through redundant replication servers.</td><td>AWS Glue connects to the data source of your preference, whether it is in an S3 file, RDS table, or so on. AWS Glue also provides default retry behavior that will retry all failures three times before sending out an error notification. You can set up Amazon SNS notifications via Amazon CloudWatch actions.</td></tr>\n",
    " <tr><td>cost</td><td>You pay per shard hour and per PUT payload unit. Optionally, there are fees associated with extended data retention and enhanced fan-out, if you choose to use those features.</td><td>You pay for the volume of data you ingest using the service and for any data format conversions.</td><td>You pay for compute resources (depending on instance type) used during the migration process and any additional log storage. There are also potential data transfer fees.</td><td>With AWS Glue, you pay an hourly rate, billed by the second, for crawlers (discovering data) and ETL jobs (processing and loading data).</td></tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform and filter data during collection process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Amazon Kinesis Data Firehose</h5>\n",
    "<p>When you enable Kinesis Data Firehose data transformation, Kinesis Data Firehose buffers incoming data and invokes the specified AWS Lambda function with each buffered batch asynchronously. The transformed data is sent from Lambda to Kinesis Data Firehose for buffering and then delivered to the destination. You can also choose to enable source record backup, which backs up all untransformed records to your S3 bucket concurrently while delivering transformed records to the destination.</p>\n",
    "<h5>AWS Lambda</h5>\n",
    "<p>You can also use an AWS Lambda function separately to provide format conversion, transformation, and filtering for the data in your stream. &nbsp;Using a Lambda function for preprocessing records is useful in the following scenarios:Transforming records from other formats, Expanding data, Data enrichment, Data Filtering and String transformations.</p>\n",
    "<h5>AWS Database Migration Service</h5> \n",
    "<p>AWS DMS offers enhanced data transformation capabilities for table and schema migrations. You can change schema, table, and column names during migration using explicit selections, specify the name of the tablespace in which you want the table or table index to be created for an Oracle target, and update the primary key and unique key for a table on any <a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">supported target</a>. It also supports CSV format by default and Apache Parquet format (.parquet) when migrating data to Amazon S3 for more compact storage and faster queries. This format(parquet) is supported for both full load and change data capture (CDC).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage and Data Management (22%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon RDS:<div class=\"fr-view\"><p>General characteristics:&nbsp;</p><ul><li><strong>Fast</strong><strong>:</strong>&nbsp; Choose between two SSD-backed storage options for high performance OLTP storage.</li><li><strong>Scales vertically</strong><strong>:</strong> Amazon RDS is bounded by instance and storage size limitations.</li><li><strong>Reliable and durable</strong><strong>:</strong>&nbsp;Offers Multi-AZ and automated backups, snapshots, and failover.&nbsp;</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon DynamoDB\n",
    "<div class=\"fr-view\"><p>General characteristics:&nbsp;</p><ul><li><strong>Fast</strong><strong>:</strong> Consistent single-digit millisecond latency at any scale.&nbsp;</li><li><strong>Scales horizontally</strong><strong>:</strong> Useful for storing unbounded data, providing low cost and performance regardless of size.&nbsp;</li><li><strong>Reliable and durable</strong><strong>: </strong>Data is replicated across three fault-tolerant Availability Zones with fine grained access control. &nbsp;Offers global tables for multi-region replication.&nbsp;</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon ElastiCache\n",
    "<div class=\"fr-view\"><p>General characteristics:&nbsp;</p><ul><li><strong>Extreme performance</strong><strong>: </strong>In-memory data store and cache using optimized stack to deliver sub-millisecond response times.&nbsp;</li><li><strong>Reliable and durable</strong><strong>:</strong> ElastiCache for Redis offers multi-AZ storage with automatic failover.&nbsp;</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Neptune\n",
    "<div class=\"fr-view\"><p>General characteristics:&nbsp;</p><ul><li><strong>Fast and scalable:</strong> Store billions of relationships and query with milliseconds latency.</li><li><strong>Scales vertically:&nbsp;</strong>Neptune is bounded by instance and storage size limitations.&nbsp;</li><li><strong>Reliable and durable:</strong> Six replicas of your data across three Availability Zones with full backup and restore.&nbsp;</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Redshift\n",
    "<div class=\"fr-view\"><p>General characteristics:&nbsp;</p><ul><li><strong>Fast:</strong> Columnar storage technology to improve I/O efficiency and parallelize queries. Data loads linearly. Redshift also provides fastest query results using higher cost storage.</li><li><strong>Reliable and durable:</strong> Replicates your data within your data warehouse cluster and continuously backs up your data to Amazon S3, which is designed for eleven nines of durability.</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon S3\n",
    "<div class=\"fr-view\"><p>General characteristics:&nbsp;</p><ul><li><strong>Fast:</strong> Amazon S3 storage provides a fast reliable platform for querying structured and semistructured data. Redshift spectrum and Amazon Athena are able to query data without moving it from Amazon S3 at high speed with low latency.</li><li><strong>Reliable and durable:</strong> Data is always replicated across three Availability Zones in the same region. Amazon S3 also offers same-region and &nbsp;cross-region replication for greater durability.&nbsp;</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "\ttable.tableizer-table {\n",
    "\t\tfont-size: 12px;\n",
    "\t\tborder: 1px solid #CCC; \n",
    "\t\tfont-family: Arial, Helvetica, sans-serif;\n",
    "\t} \n",
    "\t.tableizer-table td {\n",
    "\t\tpadding: 4px;\n",
    "\t\tmargin: 3px;\n",
    "\t\tborder: 1px solid #CCC;\n",
    "\t}\n",
    "\t.tableizer-table th {\n",
    "\t\tbackground-color: #104E8B; \n",
    "\t\tcolor: #FFF;\n",
    "\t\tfont-weight: bold;\n",
    "\t}\n",
    "</style>\n",
    "<table class=\"tableizer-table\">\n",
    "<thead><tr class=\"tableizer-firstrow\"><th></th><th>Amazon ElastiCache</th><th>Amazon DynamoDB + Amazon DynamoDB Accelerator (DAX)</th><th>Amazon Aurora</th><th>Amazon RDS </th><th>Amazon Neptune</th><th>Amazon S3 + Amazon S3 Glacier </th><th>Amazon Redshift</th></tr></thead><tbody>\n",
    " <tr><td>Use cases</td><td>In-memory caching</td><td>K/V lookups, document store</td><td>OLTP, transactional</td><td>OLTP, transactional</td><td>Graph</td><td>File Store</td><td>cloud data warehouse</td></tr>\n",
    " <tr><td>Throughput</td><td>Ultra-high request rate, Ultra-low latency</td><td>Ultra-high request rate, Ultra-low to low latency </td><td>Very high request rate, low latency</td><td>High request rate, low latency</td><td>Medium request rate, low latency</td><td>High throughput</td><td>High request rate, low latency</td></tr>\n",
    " <tr><td>Shape</td><td>K/V</td><td>K/V and document</td><td>Relational</td><td>Relational</td><td>Node/edges</td><td>Files</td><td>Nodes/Columnar Storage</td></tr>\n",
    " <tr><td>Size</td><td>GB</td><td>TB, PB (no limits)</td><td>GB, mid TB</td><td>GB, low TB</td><td>GB, mid TB</td><td>GB, TB, PB, EB (no limits)</td><td>TB, PB (no limits)</td></tr>\n",
    " <tr><td>Cost/GB</td><td><span>&#36;</span><span>&#36;</span></td> <td><span>&#162;</span><span>&#162;</span> - <span>&#36;</span><span>&#36;</span></td> <td><span>&#162;</span><span>&#162;</span></td><td><span>&#162;</span><span>&#162;</span></td><td><span>&#162;</span><span>&#162;</span></td><td><span>&#162;</span></td><td><span>&#162;</span><span>&#162;</span> - <span>&#36;</span><span>&#36;</span></td></tr>\n",
    " <tr><td>Availability</td><td>2 AZ</td><td>3 AZ</td><td>3 AZ</td><td>2 AZ</td><td>3 AZ</td><td>3 AZ</td><td>backup is replicated to S3 (3 AZ)</td></tr>\n",
    " <tr><td>Amazon Virtual Private Cloud (VPC)support</td><td>Inside VPC</td><td>DynamoDB has a VPC endpoint and DAX is inside VPC</td><td>Inside VPC</td><td>Inside VPC</td><td>Inside VPC</td><td>VPC endpoint</td><td>inside VPC</td></tr>\n",
    "<tr><td>Data Storage Type</td><td>Operational</td><td>Operational</td><td>Operational</td><td>Operational</td><td>Operational</td><td>Analytic</td><td>Analytic</td></tr>\n",
    "<tr><td>How long do you need to store the data</td><td>Storing transient data</td><td>Storing long-term data(DynamoDB) / Storing transient data(DAX)</td><td>Storing long-term data</td><td>Storing long-term data</td><td>Storing long-term data</td><td>Storing long-term data(S3) / Storing archived data(Glacier) </td><td>Storing long-term data</td></tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What type of data do you need to store?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%;\"><thead><tr><th style=\"width:26.6549%;background-color:rgb(13, 78, 70);\"><span style=\"color:rgb(255, 255, 255);\"><strong>Attribute</strong></span></th><th style=\"width:39.968%;background-color:rgb(13, 78, 70);\"><span style=\"color:rgb(255, 255, 255);font-weight:bold;\">Data Warehouse</span><br></th><th style=\"width:33.3333%;background-color:rgb(13, 78, 70);\"><span style=\"color:rgb(255, 255, 255);font-weight:bold;\">Data Lake</span></th></tr></thead><tbody><tr><td style=\"text-align:center;width:26.6549%;background-color:rgb(239, 239, 239);\"><span style=\"color:rgb(49, 53, 55);\"><strong>Data</strong></span></td><td style=\"text-align:center;width:39.968%;\"><strong>Relational from transactional systems, operational databases, and line of business applications</strong></td><td style=\"text-align:center;width:33.3333%;\"><strong>Nonrelational and relational from IoT devices, web sites, mobile apps, social media, and corporate applications</strong></td></tr><tr><td style=\"text-align:center;width:26.6549%;background-color:rgb(239, 239, 239);\"><strong><span style=\"color:rgb(49, 53, 55);\">Schema</span></strong><span style=\"color:rgb(255, 255, 255);\"><br></span></td><td style=\"text-align:center;width:39.968%;\"><strong>Designed prior to the implementation of the data warehouse (schema-on-write)</strong><br></td><td style=\"text-align:center;width:33.3333%;\"><strong>Written at the time of analysis (schema-on-read)</strong><br></td></tr><tr><td style=\"text-align:center;width:26.6549%;background-color:rgb(239, 239, 239);\"><span style=\"color:rgb(49, 53, 55);\"><strong>Price/performance</strong></span><span style=\"color:rgb(255, 255, 255);\"><strong><br></strong></span></td><td style=\"text-align:center;width:39.968%;\"><strong>Fastest query results using higher cost storage</strong></td><td style=\"text-align:center;width:33.3333%;\"><strong>Query results getting faster using low-cost storage</strong></td></tr><tr><td style=\"text-align:center;width:26.6549%;background-color:rgb(239, 239, 239);\"><strong><span style=\"color:rgb(49, 53, 55);\">Data quality</span></strong><span style=\"color:rgb(255, 255, 255);\"><br></span></td><td style=\"text-align:center;width:39.968%;\"><strong>Highly curated data,</strong><br><strong>serves as the central version of the truth</strong><br></td><td style=\"text-align:center;width:33.3333%;\"><strong>Raw data,</strong><br><strong>may or may not be curated</strong><br></td></tr><tr><td style=\"text-align:center;width:26.6549%;background-color:rgb(239, 239, 239);\"><strong><span style=\"color:rgb(49, 53, 55);\">Analytics</span></strong><span style=\"color:rgb(255, 255, 255);\"><br></span></td><td style=\"text-align:center;width:39.968%;\"><strong>Batch reporting, ad-hoc analysis, business intelligence, and data visualization</strong><br></td><td style=\"text-align:center;width:33.3333%;\"><strong>Machine learning, predictive analytics, data discovery, and profiling</strong><br></td></tr></tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing (24 %)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
