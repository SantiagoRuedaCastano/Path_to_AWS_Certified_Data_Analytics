{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path to AWS Certified Data Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"src/Domains.png\" width=\"779\" height=\"196\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect (18%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "\ttable.tableizer-table {\n",
    "\t\tfont-size: 12px;\n",
    "\t\tborder: 1px solid #CCC; \n",
    "\t\tfont-family: Arial, Helvetica, sans-serif;\n",
    "\t} \n",
    "\t.tableizer-table td {\n",
    "\t\tpadding: 4px;\n",
    "\t\tmargin: 3px;\n",
    "\t\tborder: 1px solid #CCC;\n",
    "\t}\n",
    "\t.tableizer-table th {\n",
    "\t\tbackground-color: #104E8B; \n",
    "\t\tcolor: #FFF;\n",
    "\t\tfont-weight: bold;\n",
    "\t}\n",
    "</style>\n",
    "<table class=\"tableizer-table\" > \n",
    "           <tbody> \n",
    "            <tr class=\"tableizer-firstrow\"> \n",
    "             <th width=\"15%\">&nbsp;</th> \n",
    "             <th style=\"text-align: center;\">Batch processing</th> \n",
    "             <th style=\"text-align: center;\">Stream processing<br> </th> \n",
    "            </tr> \n",
    "            <tr> \n",
    "             <td width=\"15%\">Data scope</td> \n",
    "             <td style=\"text-align: left;\">Queries or processing over all or most of the data in the dataset.</td> \n",
    "             <td style=\"text-align: left;\">Queries or processing over data within a rolling time window, or on just the most recent data record.</td> \n",
    "            </tr> \n",
    "            <tr> \n",
    "             <td width=\"15%\">Data size</td> \n",
    "             <td style=\"text-align: left;\">Large batches of data.</td> \n",
    "             <td style=\"text-align: left;\">Individual records or micro batches consisting of a few records.</td> \n",
    "            </tr> \n",
    "            <tr> \n",
    "             <td width=\"15%\">Performance</td> \n",
    "             <td style=\"text-align: left;\">Latencies in minutes to hours.</td> \n",
    "             <td style=\"text-align: left;\">Requires latency in the order of seconds or milliseconds.</td> \n",
    "            </tr> \n",
    "            <tr> \n",
    "             <td width=\"15%\">Analyses</td> \n",
    "             <td style=\"text-align: left;\">Complex analytics.</td> \n",
    "             <td style=\"text-align: left;\">Simple response functions, aggregates, and rolling metrics.</td> \n",
    "            </tr> \n",
    "           </tbody> \n",
    "          </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>At most once</h5>\n",
    "At most once means that a message will never be delivered more than one time. However, a message could be lost. Note that, this\n",
    "pattern is only accepted when the data itself is low value or loses value if it is not immediately processed.\n",
    "<h5>At least once</h5>\n",
    "delivery replays recent events starting from an acknowledged (known processed) event.\n",
    "This approach presents some data more than once to the processing pipeline. The typical implementation\n",
    "returns at-least-once delivery checkpoints to a safe point (so you know that they have been processed).\n",
    "<h5>Exactly Once</h5> \n",
    "This type of processing is the ideal because each event is processed exactly once. It avoids the difficult side\n",
    "effects and considerations raised by the other two deliveries. You can achieve\n",
    "exactly-once semantics using idempotency in combination with at-least-once delivery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review operational characteristics of AWS ingestion services\n",
    "\n",
    "<style type=\"text/css\">\n",
    "\ttable.tableizer-table {\n",
    "\t\tfont-size: 12px;\n",
    "\t\tborder: 1px solid #CCC; \n",
    "\t\tfont-family: Arial, Helvetica, sans-serif;\n",
    "\t} \n",
    "\t.tableizer-table td {\n",
    "\t\tpadding: 4px;\n",
    "\t\tmargin: 3px;\n",
    "\t\tborder: 1px solid #CCC;\n",
    "\t}\n",
    "\t.tableizer-table th {\n",
    "\t\tbackground-color: #104E8B; \n",
    "\t\tcolor: #FFF;\n",
    "\t\tfont-weight: bold;\n",
    "\t}\n",
    "</style>\n",
    "<table class=\"tableizer-table\">\n",
    "<thead><tr class=\"tableizer-firstrow\"><th></th><th>Amazon DynamoDB Streams</th><th>Amazon Kinesis Data Stream(KDS)</th><th>Amazon Kinesis Data Firehose</th><th>Amazon MSK</th><th>Apache Kafka(on EC2)</th><th>Amazon SQS Standard</th><th>Amazon SQS FIFO</th></tr></thead><tbody>\n",
    " <tr><td>AWS Managed</td><td>Yes</td><td>Yes</td><td>Yes</td><td>Yes</td><td>No</td><td>Yes</td><td>Yes</td></tr>\n",
    " <tr><td>Use cases</td><td><ul><li>Replication</li><li>Mobile app</li><li>Global multi-player game</li></ul></td><td> real-time data streaming</td><td>Loader streaming data into data lakes, data stores and analytics tools</td><td>real-time streaming data pipelines and applications</td><td>real-time streaming data pipelines and applications</td><td>Queue</td><td>Queue</td></tr>\n",
    " <tr><td>Guaranteed ordering</td><td>Yes</td><td>Yes</td><td>No</td><td>Yes</td><td>Yes</td><td>No</td><td>Yes</td></tr>\n",
    " <tr><td>Delivery(deduping)</td><td>Exactly once</td><td>At least once</td><td>At least once</td><td>At least once</td><td>At least once</td><td>At least once</td><td>Exactly once</td></tr>\n",
    " <tr><td>Data retention period</td><td>24 hours</td><td>7 days</td><td>N/A</td><td>Configurable</td><td>Configurable</td><td>14 days</td><td>14 days</td></tr>\n",
    " <tr><td>Availability</td><td>3 AZ</td><td>3 AZ</td><td>3 AZ</td><td>Configurable Multi AZ</td><td>Configurable Multi AZ</td><td>3 AZ</td><td>3 AZ</td></tr>\n",
    " <tr><td>Scale/throughput</td><td>No limit / Automatic</td><td>No limit / shards</td><td>No limit / Automatic</td><td>up to 30 / brokers</td><td>No limit / nodes</td><td>No limit / Automatic</td><td>300 TPS / queue</td></tr>\n",
    " <tr><td>Parallel Consumption</td><td>Yes</td><td>Yes</td><td>No</td><td>Yes</td><td>Yes</td><td>No</td><td>No</td></tr>\n",
    " <tr><td>Row/Object Size</td><td>400 KB</td><td>1 MB</td><td>Destination row / object size</td><td>Varies</td><td>Varies</td><td>256 KB</td><td>256 KB</td></tr>\n",
    " <tr><td>Stream MapReduce</td><td>Yes</td><td>Yes</td><td>N/A</td><td>Yes</td><td>Yes</td><td>N/A</td><td>N/A</td></tr>\n",
    "<tr><td>Cost</td><td>Higher</td><td>Low</td><td>Low</td><td>Low</td><td>Low(+Admin)</td><td>Low-Medium</td><td>Low-Medium</td></tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon MSK: up to 30 brokers, but, you can request a limit increase in the AWS Support Center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <ol>\n",
    "  <li>How quickly do you need the analytics results?  In real time, in seconds, or is an hour a more appropriate time frame?</li>\n",
    "  <li>Where is the data coming from?</li>\n",
    "</ol> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing ingestion services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "\ttable.tableizer-table {\n",
    "\t\tfont-size: 12px;\n",
    "\t\tborder: 1px solid #CCC; \n",
    "\t\tfont-family: Arial, Helvetica, sans-serif;\n",
    "\t} \n",
    "\t.tableizer-table td {\n",
    "\t\tpadding: 4px;\n",
    "\t\tmargin: 3px;\n",
    "\t\tborder: 1px solid #CCC;\n",
    "\t}\n",
    "\t.tableizer-table th {\n",
    "\t\tbackground-color: #104E8B; \n",
    "\t\tcolor: #FFF;\n",
    "\t\tfont-weight: bold;\n",
    "\t}\n",
    "</style>\n",
    "<table class=\"tableizer-table\">\n",
    "<thead><tr class=\"tableizer-firstrow\"><th></th><th>Kinesis Data Streams</th><th>Kinesis Data Firehose</th><th>AWS DMS</th><th>AWS Glue</th></tr></thead><tbody>\n",
    " <tr><td>scale and throughput</td><td>Each shard can support up to 1,000 PUT records per second. However, you can increase the number of shards limitlessly. One shard provides a capacity of 1 MB/sec data input and 2 MB/sec data output</td><td>Kinesis Data Firehose will automatically scale to match the throughput of your data, without any manual intervention or developer overhead.</td><td>AWS DMS uses Amazon EC2 instances as the replication instance. You can scale up or down your replication instance.</td><td>AWS Glue uses a scale-out Apache Spark environment to load your data into its destination. To scale out, you specify the number of DPUs (data processing units) that you want to allocate to your ETL jobs.</td></tr>\n",
    " <tr><td>fault tolerance</td><td>3 AZ</td><td>3 AZ</td><td>You have the option of enabling Multi-AZ which provides a replication stream that is fault-tolerant through redundant replication servers.</td><td>AWS Glue connects to the data source of your preference, whether it is in an S3 file, RDS table, or so on. AWS Glue also provides default retry behavior that will retry all failures three times before sending out an error notification. You can set up Amazon SNS notifications via Amazon CloudWatch actions.</td></tr>\n",
    " <tr><td>cost</td><td>You pay per shard hour and per PUT payload unit. Optionally, there are fees associated with extended data retention and enhanced fan-out, if you choose to use those features.</td><td>You pay for the volume of data you ingest using the service and for any data format conversions.</td><td>You pay for compute resources (depending on instance type) used during the migration process and any additional log storage. There are also potential data transfer fees.</td><td>With AWS Glue, you pay an hourly rate, billed by the second, for crawlers (discovering data) and ETL jobs (processing and loading data).</td></tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform and filter data during collection process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Amazon Kinesis Data Firehose</h5>\n",
    "<p>When you enable Kinesis Data Firehose data transformation, Kinesis Data Firehose buffers incoming data and invokes the specified AWS Lambda function with each buffered batch asynchronously. The transformed data is sent from Lambda to Kinesis Data Firehose for buffering and then delivered to the destination. You can also choose to enable source record backup, which backs up all untransformed records to your S3 bucket concurrently while delivering transformed records to the destination.</p>\n",
    "<h5>AWS Lambda</h5>\n",
    "<p>You can also use an AWS Lambda function separately to provide format conversion, transformation, and filtering for the data in your stream. &nbsp;Using a Lambda function for preprocessing records is useful in the following scenarios:Transforming records from other formats, Expanding data, Data enrichment, Data Filtering and String transformations.</p>\n",
    "<h5>AWS Database Migration Service</h5> \n",
    "<p>AWS DMS offers enhanced data transformation capabilities for table and schema migrations. You can change schema, table, and column names during migration using explicit selections, specify the name of the tablespace in which you want the table or table index to be created for an Oracle target, and update the primary key and unique key for a table on any <a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">supported target</a>. It also supports CSV format by default and Apache Parquet format (.parquet) when migrating data to Amazon S3 for more compact storage and faster queries. This format(parquet) is supported for both full load and change data capture (CDC).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage and Data Management (22%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon RDS:<div class=\"fr-view\"><p>General characteristics:&nbsp;</p><ul><li><strong>Fast</strong><strong>:</strong>&nbsp; Choose between two SSD-backed storage options for high performance OLTP storage.</li><li><strong>Scales vertically</strong><strong>:</strong> Amazon RDS is bounded by instance and storage size limitations.</li><li><strong>Reliable and durable</strong><strong>:</strong>&nbsp;Offers Multi-AZ and automated backups, snapshots, and failover.&nbsp;</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon DynamoDB\n",
    "<div class=\"fr-view\"><p>General characteristics:&nbsp;</p><ul><li><strong>Fast</strong><strong>:</strong> Consistent single-digit millisecond latency at any scale.&nbsp;</li><li><strong>Scales horizontally</strong><strong>:</strong> Useful for storing unbounded data, providing low cost and performance regardless of size.&nbsp;</li><li><strong>Reliable and durable</strong><strong>: </strong>Data is replicated across three fault-tolerant Availability Zones with fine grained access control. &nbsp;Offers global tables for multi-region replication.&nbsp;</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon ElastiCache\n",
    "<div class=\"fr-view\"><p>General characteristics:&nbsp;</p><ul><li><strong>Extreme performance</strong><strong>: </strong>In-memory data store and cache using optimized stack to deliver sub-millisecond response times.&nbsp;</li><li><strong>Reliable and durable</strong><strong>:</strong> ElastiCache for Redis offers multi-AZ storage with automatic failover.&nbsp;</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Neptune\n",
    "<div class=\"fr-view\"><p>General characteristics:&nbsp;</p><ul><li><strong>Fast and scalable:</strong> Store billions of relationships and query with milliseconds latency.</li><li><strong>Scales vertically:&nbsp;</strong>Neptune is bounded by instance and storage size limitations.&nbsp;</li><li><strong>Reliable and durable:</strong> Six replicas of your data across three Availability Zones with full backup and restore.&nbsp;</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Redshift\n",
    "<div class=\"fr-view\"><p>General characteristics:&nbsp;</p><ul><li><strong>Fast:</strong> Columnar storage technology to improve I/O efficiency and parallelize queries. Data loads linearly. Redshift also provides fastest query results using higher cost storage.</li><li><strong>Reliable and durable:</strong> Replicates your data within your data warehouse cluster and continuously backs up your data to Amazon S3, which is designed for eleven nines of durability.</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon S3\n",
    "<div class=\"fr-view\"><p>General characteristics:&nbsp;</p><ul><li><strong>Fast:</strong> Amazon S3 storage provides a fast reliable platform for querying structured and semistructured data. Redshift spectrum and Amazon Athena are able to query data without moving it from Amazon S3 at high speed with low latency.</li><li><strong>Reliable and durable:</strong> Data is always replicated across three Availability Zones in the same region. Amazon S3 also offers same-region and &nbsp;cross-region replication for greater durability.&nbsp;</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "\ttable.tableizer-table {\n",
    "\t\tfont-size: 12px;\n",
    "\t\tborder: 1px solid #CCC; \n",
    "\t\tfont-family: Arial, Helvetica, sans-serif;\n",
    "\t} \n",
    "\t.tableizer-table td {\n",
    "\t\tpadding: 4px;\n",
    "\t\tmargin: 3px;\n",
    "\t\tborder: 1px solid #CCC;\n",
    "\t}\n",
    "\t.tableizer-table th {\n",
    "\t\tbackground-color: #104E8B; \n",
    "\t\tcolor: #FFF;\n",
    "\t\tfont-weight: bold;\n",
    "\t}\n",
    "</style>\n",
    "<table class=\"tableizer-table\">\n",
    "<thead><tr class=\"tableizer-firstrow\"><th></th><th>Amazon ElastiCache</th><th>Amazon DynamoDB + Amazon DynamoDB Accelerator (DAX)</th><th>Amazon Aurora</th><th>Amazon RDS </th><th>Amazon Neptune</th><th>Amazon S3 + Amazon S3 Glacier </th><th>Amazon Redshift</th></tr></thead><tbody>\n",
    " <tr><td>Use cases</td><td>In-memory caching</td><td>K/V lookups, document store</td><td>OLTP, transactional</td><td>OLTP, transactional</td><td>Graph</td><td>File Store</td><td>cloud data warehouse</td></tr>\n",
    " <tr><td>Throughput</td><td>Ultra-high request rate, Ultra-low latency</td><td>Ultra-high request rate, Ultra-low to low latency </td><td>Very high request rate, low latency</td><td>High request rate, low latency</td><td>Medium request rate, low latency</td><td>High throughput</td><td>High request rate, low latency</td></tr>\n",
    " <tr><td>Shape</td><td>K/V</td><td>K/V and document</td><td>Relational</td><td>Relational</td><td>Node/edges</td><td>Files</td><td>Nodes/Columnar Storage</td></tr>\n",
    " <tr><td>Size</td><td>GB</td><td>TB, PB (no limits)</td><td>GB, mid TB</td><td>GB, low TB</td><td>GB, mid TB</td><td>GB, TB, PB, EB (no limits)</td><td>TB, PB (no limits)</td></tr>\n",
    " <tr><td>Cost/GB</td><td><span>&#36;</span><span>&#36;</span></td> <td><span>&#162;</span><span>&#162;</span> - <span>&#36;</span><span>&#36;</span></td> <td><span>&#162;</span><span>&#162;</span></td><td><span>&#162;</span><span>&#162;</span></td><td><span>&#162;</span><span>&#162;</span></td><td><span>&#162;</span></td><td><span>&#162;</span><span>&#162;</span> - <span>&#36;</span><span>&#36;</span></td></tr>\n",
    " <tr><td>Availability</td><td>2 AZ</td><td>3 AZ</td><td>3 AZ</td><td>2 AZ</td><td>3 AZ</td><td>3 AZ</td><td>backup is replicated to S3 (3 AZ)</td></tr>\n",
    " <tr><td>Amazon Virtual Private Cloud (VPC)support</td><td>Inside VPC</td><td>DynamoDB has a VPC endpoint and DAX is inside VPC</td><td>Inside VPC</td><td>Inside VPC</td><td>Inside VPC</td><td>VPC endpoint</td><td>inside VPC</td></tr>\n",
    "<tr><td>Data Storage Type</td><td>Operational</td><td>Operational</td><td>Operational</td><td>Operational</td><td>Operational</td><td>Analytic</td><td>Analytic</td></tr>\n",
    "<tr><td>How long do you need to store the data</td><td>Storing transient data</td><td>Storing long-term data(DynamoDB) / Storing transient data(DAX)</td><td>Storing long-term data</td><td>Storing long-term data</td><td>Storing long-term data</td><td>Storing long-term data(S3) / Storing archived data(Glacier) </td><td>Storing long-term data</td></tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What type of data do you need to store?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%;\"><thead><tr><th style=\"width:26.6549%;background-color:rgb(13, 78, 70);\"><span style=\"color:rgb(255, 255, 255);\"><strong>Attribute</strong></span></th><th style=\"width:39.968%;background-color:rgb(13, 78, 70);\"><span style=\"color:rgb(255, 255, 255);font-weight:bold;\">Data Warehouse</span><br></th><th style=\"width:33.3333%;background-color:rgb(13, 78, 70);\"><span style=\"color:rgb(255, 255, 255);font-weight:bold;\">Data Lake</span></th></tr></thead><tbody><tr><td style=\"text-align:center;width:26.6549%;background-color:rgb(239, 239, 239);\"><span style=\"color:rgb(49, 53, 55);\"><strong>Data</strong></span></td><td style=\"text-align:center;width:39.968%;\"><strong>Relational from transactional systems, operational databases, and line of business applications</strong></td><td style=\"text-align:center;width:33.3333%;\"><strong>Nonrelational and relational from IoT devices, web sites, mobile apps, social media, and corporate applications</strong></td></tr><tr><td style=\"text-align:center;width:26.6549%;background-color:rgb(239, 239, 239);\"><strong><span style=\"color:rgb(49, 53, 55);\">Schema</span></strong><span style=\"color:rgb(255, 255, 255);\"><br></span></td><td style=\"text-align:center;width:39.968%;\"><strong>Designed prior to the implementation of the data warehouse (schema-on-write)</strong><br></td><td style=\"text-align:center;width:33.3333%;\"><strong>Written at the time of analysis (schema-on-read)</strong><br></td></tr><tr><td style=\"text-align:center;width:26.6549%;background-color:rgb(239, 239, 239);\"><span style=\"color:rgb(49, 53, 55);\"><strong>Price/performance</strong></span><span style=\"color:rgb(255, 255, 255);\"><strong><br></strong></span></td><td style=\"text-align:center;width:39.968%;\"><strong>Fastest query results using higher cost storage</strong></td><td style=\"text-align:center;width:33.3333%;\"><strong>Query results getting faster using low-cost storage</strong></td></tr><tr><td style=\"text-align:center;width:26.6549%;background-color:rgb(239, 239, 239);\"><strong><span style=\"color:rgb(49, 53, 55);\">Data quality</span></strong><span style=\"color:rgb(255, 255, 255);\"><br></span></td><td style=\"text-align:center;width:39.968%;\"><strong>Highly curated data,</strong><br><strong>serves as the central version of the truth</strong><br></td><td style=\"text-align:center;width:33.3333%;\"><strong>Raw data,</strong><br><strong>may or may not be curated</strong><br></td></tr><tr><td style=\"text-align:center;width:26.6549%;background-color:rgb(239, 239, 239);\"><strong><span style=\"color:rgb(49, 53, 55);\">Analytics</span></strong><span style=\"color:rgb(255, 255, 255);\"><br></span></td><td style=\"text-align:center;width:39.968%;\"><strong>Batch reporting, ad-hoc analysis, business intelligence, and data visualization</strong><br></td><td style=\"text-align:center;width:33.3333%;\"><strong>Machine learning, predictive analytics, data discovery, and profiling</strong><br></td></tr></tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizing Amazon Redshift "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Redshift is a massively parallel processing (MPP) system. Each Amazon Redshift cluster has one leader node, two or more compute nodes, and each compute node has a number of slices. Each node slice is an independent partition of data. The slices can perform operations in parallel to complete operations significantly faster than systems that do not use slices. Because of the massive amount of data involved in data warehousing, you must design your database to take full advantage of every performance optimization. The four types of Amazon Redshift optimizations that this section will cover are data distribution styles, sort key best practices, compression encoding implementations, and data size optimizations.\n",
    "##### Amazon Redshift data redistribution\n",
    "\n",
    "When you load data into a table, Amazon Redshift distributes the table's rows to the compute nodes and slices according to the distribution style that you chose when you created the table. The distribution strategy that you choose for your database has important consequences for query performance, storage requirements, data loading, and maintenance. By choosing the best distribution style for each table, you can balance your data distribution and significantly improve overall system performance. Click each of the following markers to learn more about the four different distribution styles that Redshift supports. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KEY distribution\n",
    "<p>The rows are distributed according to the values in one column. The leader node attempts to place matching values on the same node slice. If you distribute a pair of tables on the joining keys, the leader node collocates the rows on the slices according to the values in the joining columns so that matching values from the common columns are physically stored together.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVEN distribution\n",
    "<p>The leader node distributes the rows across the slices in a round-robin fashion, regardless of the values in any particular column. EVEN distribution is appropriate when a table does not participate in joins or when there is not a clear choice between KEY distribution and ALL distribution.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALL distribution\n",
    "<p>A copy of the entire table is distributed to every node. Where EVEN distribution or KEY distribution place only a portion of a table's rows on each node, ALL distribution ensures that every row is collocated for every join that the table participates in. &nbsp;ALL distribution multiplies the storage required by the number of nodes in the cluster, and so it takes much longer to load, update, or insert data into multiple tables. ALL distribution is appropriate only for relatively slow-moving tables; that is, tables that are not updated frequently or extensively. Small dimension tables do not benefit significantly from ALL distribution, because the cost of redistribution is low.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUTO distribution\n",
    "<p>With AUTO distribution, Amazon Redshift assigns an optimal distribution style based on the size of the table data. For example, Amazon Redshift initially assigns ALL distribution to a small table, then changes to EVEN distribution when the table grows larger. When a table is changed from ALL to EVEN distribution, storage utilization might change slightly. The change in distribution occurs in the background, in a few seconds. Amazon Redshift never changes the distribution style from EVEN to ALL.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choosing a sort key in Amazon Redshift  \n",
    "\n",
    "When you create a table, you can define one or more of its columns as sort keys. When data is initially loaded into the empty table, the rows are stored on disk in sorted order. Information about sort key columns is passed to the query planner, and the planner uses this information to construct plans that exploit the way that the data is sorted. You can specify either a compound or interleaved sort key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interleaved\n",
    "<p>An interleaved sort key gives equal weight to each column in the sort key, so query predicates can use any subset of the columns that make up the sort key, in any order. The performance improvements you gain by implementing an interleaved sort key should be weighed against increased load and vacuum times. Donâ€™t use an interleaved sort key on columns with monotonically increasing attributes, such as identity columns, dates, or timestamps.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compound\n",
    "<p>A compound sort key is more efficient when query predicates use a prefix, which is a subset of the sort key columns in order.&nbsp;<br>Compound sort keys might speed up joins, GROUP BY and ORDER BY operations, and window functions that use PARTITION BY and ORDER BY. For example, a merge join, which is often faster than a hash join, is feasible when the data is distributed and presorted on the joining columns. Compound sort keys also help improve compression.<br>The performance benefits of compound sorting decrease when queries depend only on secondary sort columns, without referencing the primary columns. COMPOUND is the default sort type.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compression encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"fr-view\"><p>A compression encoding specifies the type of compression that is applied to a column of data values as rows are added to a table. Learn more about the different compression types: </p><ul><li><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_Raw_encoding.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">Raw Encoding</a></li><li><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/az64-encoding.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">AZ64 Encoding</a></li><li><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_Byte_dictionary_encoding.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">Byte-Dictionary Encoding</a></li><li><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_Delta_encoding.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">Delta Encoding</a></li><li><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/lzo-encoding.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">LZO Encoding</a></li><li><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_MostlyN_encoding.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">Mostly Encoding</a></li><li><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_Runlength_encoding.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">Runlength Encoding</a></li><li><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_Text255_encoding.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">Text255 and Text32k Encodings</a></li><li><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/zstd-encoding.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">Zstandard Encoding</a></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"fr-view\"><p>If no compression is specified in a CREATE TABLE or ALTER TABLE statement, Amazon Redshift automatically assigns compression encoding as follows:</p><ul><li><p>Columns that are defined as BOOLEAN, REAL, or DOUBLE PRECISION data types are assigned RAW compression.</p></li><li><p>Columns that are defined as SMALLINT, INTEGER, BIGINT, DECIMAL, DATE, TIMESTAMP, or TIMESTAMPTZ data types are assigned AZ64 compression.</p></li><li><p>Columns that are defined as CHAR or VARCHAR data types are assigned LZO compression.</p></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizing size of data in Amazon Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"fr-view\"><p>Best practices for optimizing data size:&nbsp;</p><ul><li>Split your load data files so that the files are about equal size, between 1 MB and 1 GB after compression. For optimum parallelism, the ideal size is between 1 MB and 125 MB after compression.&nbsp;</li><li>The number of files should be a multiple of the number of slices in your cluster.</li><li>When loading data, we strongly recommend that you individually compress your load files using gzip, lzop, bzip2, or Zstandard when you have large datasets. See more Amazon Redshift loading best practices <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-compress-data-files.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">here</a>.</li><li>You can also apply compression encodings when creating a table, as discussed previously in the module.</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing (24 %)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
