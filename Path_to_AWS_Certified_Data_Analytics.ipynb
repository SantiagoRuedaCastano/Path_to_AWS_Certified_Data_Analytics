{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path to AWS Certified Data Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"src/Domains.png\" width=\"779\" height=\"196\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect (18%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "\ttable.tableizer-table {\n",
    "\t\tfont-size: 12px;\n",
    "\t\tborder: 1px solid #CCC; \n",
    "\t\tfont-family: Arial, Helvetica, sans-serif;\n",
    "\t} \n",
    "\t.tableizer-table td {\n",
    "\t\tpadding: 4px;\n",
    "\t\tmargin: 3px;\n",
    "\t\tborder: 1px solid #CCC;\n",
    "\t}\n",
    "\t.tableizer-table th {\n",
    "\t\tbackground-color: #104E8B; \n",
    "\t\tcolor: #FFF;\n",
    "\t\tfont-weight: bold;\n",
    "\t}\n",
    "</style>\n",
    "<table class=\"tableizer-table\" > \n",
    "           <tbody> \n",
    "            <tr class=\"tableizer-firstrow\"> \n",
    "             <th width=\"15%\">&nbsp;</th> \n",
    "             <th style=\"text-align: center;\">Batch processing</th> \n",
    "             <th style=\"text-align: center;\">Stream processing<br> </th> \n",
    "            </tr> \n",
    "            <tr> \n",
    "             <td width=\"15%\">Data scope</td> \n",
    "             <td style=\"text-align: left;\">Queries or processing over all or most of the data in the dataset.</td> \n",
    "             <td style=\"text-align: left;\">Queries or processing over data within a rolling time window, or on just the most recent data record.</td> \n",
    "            </tr> \n",
    "            <tr> \n",
    "             <td width=\"15%\">Data size</td> \n",
    "             <td style=\"text-align: left;\">Large batches of data.</td> \n",
    "             <td style=\"text-align: left;\">Individual records or micro batches consisting of a few records.</td> \n",
    "            </tr> \n",
    "            <tr> \n",
    "             <td width=\"15%\">Performance</td> \n",
    "             <td style=\"text-align: left;\">Latencies in minutes to hours.</td> \n",
    "             <td style=\"text-align: left;\">Requires latency in the order of seconds or milliseconds.</td> \n",
    "            </tr> \n",
    "            <tr> \n",
    "             <td width=\"15%\">Analyses</td> \n",
    "             <td style=\"text-align: left;\">Complex analytics.</td> \n",
    "             <td style=\"text-align: left;\">Simple response functions, aggregates, and rolling metrics.</td> \n",
    "            </tr> \n",
    "           </tbody> \n",
    "          </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>At most once</h5>\n",
    "At most once means that a message will never be delivered more than one time. However, a message could be lost. Note that, this\n",
    "pattern is only accepted when the data itself is low value or loses value if it is not immediately processed.\n",
    "<h5>At least once</h5>\n",
    "delivery replays recent events starting from an acknowledged (known processed) event.\n",
    "This approach presents some data more than once to the processing pipeline. The typical implementation\n",
    "returns at-least-once delivery checkpoints to a safe point (so you know that they have been processed).\n",
    "<h5>Exactly Once</h5> \n",
    "This type of processing is the ideal because each event is processed exactly once. It avoids the difficult side\n",
    "effects and considerations raised by the other two deliveries. You can achieve\n",
    "exactly-once semantics using idempotency in combination with at-least-once delivery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review operational characteristics of AWS ingestion services\n",
    "\n",
    "<style type=\"text/css\">\n",
    "\ttable.tableizer-table {\n",
    "\t\tfont-size: 12px;\n",
    "\t\tborder: 1px solid #CCC; \n",
    "\t\tfont-family: Arial, Helvetica, sans-serif;\n",
    "\t} \n",
    "\t.tableizer-table td {\n",
    "\t\tpadding: 4px;\n",
    "\t\tmargin: 3px;\n",
    "\t\tborder: 1px solid #CCC;\n",
    "\t}\n",
    "\t.tableizer-table th {\n",
    "\t\tbackground-color: #104E8B; \n",
    "\t\tcolor: #FFF;\n",
    "\t\tfont-weight: bold;\n",
    "\t}\n",
    "</style>\n",
    "<table class=\"tableizer-table\">\n",
    "<thead><tr class=\"tableizer-firstrow\"><th></th><th>Amazon DynamoDB Streams</th><th>Amazon Kinesis Data Stream(KDS)</th><th>Amazon Kinesis Data Firehose</th><th>Amazon MSK</th><th>Apache Kafka(on EC2)</th><th>Amazon SQS Standard</th><th>Amazon SQS FIFO</th></tr></thead><tbody>\n",
    " <tr><td>AWS Managed</td><td>Yes</td><td>Yes</td><td>Yes</td><td>Yes</td><td>No</td><td>Yes</td><td>Yes</td></tr>\n",
    " <tr><td>Use cases</td><td><ul><li>Replication</li><li>Mobile app</li><li>Global multi-player game</li></ul></td><td> real-time data streaming</td><td>Loader streaming data into data lakes, data stores and analytics tools</td><td>real-time streaming data pipelines and applications</td><td>real-time streaming data pipelines and applications</td><td>Queue</td><td>Queue</td></tr>\n",
    " <tr><td>Guaranteed ordering</td><td>Yes</td><td>Yes</td><td>No</td><td>Yes</td><td>Yes</td><td>No</td><td>Yes</td></tr>\n",
    " <tr><td>Delivery(deduping)</td><td>Exactly once</td><td>At least once</td><td>At least once</td><td>At least once</td><td>At least once</td><td>At least once</td><td>Exactly once</td></tr>\n",
    " <tr><td>Data retention period</td><td>24 hours</td><td>7 days</td><td>N/A</td><td>Configurable</td><td>Configurable</td><td>14 days</td><td>14 days</td></tr>\n",
    " <tr><td>Availability</td><td>3 AZ</td><td>3 AZ</td><td>3 AZ</td><td>Configurable Multi AZ</td><td>Configurable Multi AZ</td><td>3 AZ</td><td>3 AZ</td></tr>\n",
    " <tr><td>Scale/throughput</td><td>No limit / Automatic</td><td>No limit / shards</td><td>No limit / Automatic</td><td>up to 30 / brokers</td><td>No limit / nodes</td><td>No limit / Automatic</td><td>300 TPS / queue</td></tr>\n",
    " <tr><td>Parallel Consumption</td><td>Yes</td><td>Yes</td><td>No</td><td>Yes</td><td>Yes</td><td>No</td><td>No</td></tr>\n",
    " <tr><td>Row/Object Size</td><td>400 KB</td><td>1 MB</td><td>Destination row / object size</td><td>Varies</td><td>Varies</td><td>256 KB</td><td>256 KB</td></tr>\n",
    " <tr><td>Stream MapReduce</td><td>Yes</td><td>Yes</td><td>N/A</td><td>Yes</td><td>Yes</td><td>N/A</td><td>N/A</td></tr>\n",
    "<tr><td>Cost</td><td>Higher</td><td>Low</td><td>Low</td><td>Low</td><td>Low(+Admin)</td><td>Low-Medium</td><td>Low-Medium</td></tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon MSK: up to 30 brokers, but, you can request a limit increase in the AWS Support Center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <ol>\n",
    "  <li>How quickly do you need the analytics results?  In real time, in seconds, or is an hour a more appropriate time frame?</li>\n",
    "  <li>Where is the data coming from?</li>\n",
    "</ol> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing ingestion services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "\ttable.tableizer-table {\n",
    "\t\tfont-size: 12px;\n",
    "\t\tborder: 1px solid #CCC; \n",
    "\t\tfont-family: Arial, Helvetica, sans-serif;\n",
    "\t} \n",
    "\t.tableizer-table td {\n",
    "\t\tpadding: 4px;\n",
    "\t\tmargin: 3px;\n",
    "\t\tborder: 1px solid #CCC;\n",
    "\t}\n",
    "\t.tableizer-table th {\n",
    "\t\tbackground-color: #104E8B; \n",
    "\t\tcolor: #FFF;\n",
    "\t\tfont-weight: bold;\n",
    "\t}\n",
    "</style>\n",
    "<table class=\"tableizer-table\">\n",
    "<thead><tr class=\"tableizer-firstrow\"><th></th><th>Kinesis Data Streams</th><th>Kinesis Data Firehose</th><th>AWS DMS</th><th>AWS Glue</th></tr></thead><tbody>\n",
    " <tr><td>scale and throughput</td><td>Each shard can support up to 1,000 PUT records per second. However, you can increase the number of shards limitlessly. One shard provides a capacity of 1 MB/sec data input and 2 MB/sec data output</td><td>Kinesis Data Firehose will automatically scale to match the throughput of your data, without any manual intervention or developer overhead.</td><td>AWS DMS uses Amazon EC2 instances as the replication instance. You can scale up or down your replication instance.</td><td>AWS Glue uses a scale-out Apache Spark environment to load your data into its destination. To scale out, you specify the number of DPUs (data processing units) that you want to allocate to your ETL jobs.</td></tr>\n",
    " <tr><td>fault tolerance</td><td>3 AZ</td><td>3 AZ</td><td>You have the option of enabling Multi-AZ which provides a replication stream that is fault-tolerant through redundant replication servers.</td><td>AWS Glue connects to the data source of your preference, whether it is in an S3 file, RDS table, or so on. AWS Glue also provides default retry behavior that will retry all failures three times before sending out an error notification. You can set up Amazon SNS notifications via Amazon CloudWatch actions.</td></tr>\n",
    " <tr><td>cost</td><td>You pay per shard hour and per PUT payload unit. Optionally, there are fees associated with extended data retention and enhanced fan-out, if you choose to use those features.</td><td>You pay for the volume of data you ingest using the service and for any data format conversions.</td><td>You pay for compute resources (depending on instance type) used during the migration process and any additional log storage. There are also potential data transfer fees.</td><td>With AWS Glue, you pay an hourly rate, billed by the second, for crawlers (discovering data) and ETL jobs (processing and loading data).</td></tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform and filter data during collection process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Amazon Kinesis Data Firehose</h5>\n",
    "<p>When you enable Kinesis Data Firehose data transformation, Kinesis Data Firehose buffers incoming data and invokes the specified AWS Lambda function with each buffered batch asynchronously. The transformed data is sent from Lambda to Kinesis Data Firehose for buffering and then delivered to the destination. You can also choose to enable source record backup, which backs up all untransformed records to your S3 bucket concurrently while delivering transformed records to the destination.</p>\n",
    "<h5>AWS Lambda</h5>\n",
    "<p>You can also use an AWS Lambda function separately to provide format conversion, transformation, and filtering for the data in your stream. &nbsp;Using a Lambda function for preprocessing records is useful in the following scenarios:Transforming records from other formats, Expanding data, Data enrichment, Data Filtering and String transformations.</p>\n",
    "<h5>AWS Database Migration Service</h5> \n",
    "<p>AWS DMS offers enhanced data transformation capabilities for table and schema migrations. You can change schema, table, and column names during migration using explicit selections, specify the name of the tablespace in which you want the table or table index to be created for an Oracle target, and update the primary key and unique key for a table on any <a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">supported target</a>. It also supports CSV format by default and Apache Parquet format (.parquet) when migrating data to Amazon S3 for more compact storage and faster queries. This format(parquet) is supported for both full load and change data capture (CDC).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage and Data Management (22%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon RDS:<div class=\"fr-view\"><p>General characteristics:&nbsp;</p><ul><li><strong>Fast</strong><strong>:</strong>&nbsp; Choose between two SSD-backed storage options for high performance OLTP storage.</li><li><strong>Scales vertically</strong><strong>:</strong> Amazon RDS is bounded by instance and storage size limitations.</li><li><strong>Reliable and durable</strong><strong>:</strong>&nbsp;Offers Multi-AZ and automated backups, snapshots, and failover.&nbsp;</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon DynamoDB\n",
    "<div class=\"fr-view\"><p>General characteristics:&nbsp;</p><ul><li><strong>Fast</strong><strong>:</strong> Consistent single-digit millisecond latency at any scale.&nbsp;</li><li><strong>Scales horizontally</strong><strong>:</strong> Useful for storing unbounded data, providing low cost and performance regardless of size.&nbsp;</li><li><strong>Reliable and durable</strong><strong>: </strong>Data is replicated across three fault-tolerant Availability Zones with fine grained access control. &nbsp;Offers global tables for multi-region replication.&nbsp;</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon ElastiCache\n",
    "<div class=\"fr-view\"><p>General characteristics:&nbsp;</p><ul><li><strong>Extreme performance</strong><strong>: </strong>In-memory data store and cache using optimized stack to deliver sub-millisecond response times.&nbsp;</li><li><strong>Reliable and durable</strong><strong>:</strong> ElastiCache for Redis offers multi-AZ storage with automatic failover.&nbsp;</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Neptune\n",
    "<div class=\"fr-view\"><p>General characteristics:&nbsp;</p><ul><li><strong>Fast and scalable:</strong> Store billions of relationships and query with milliseconds latency.</li><li><strong>Scales vertically:&nbsp;</strong>Neptune is bounded by instance and storage size limitations.&nbsp;</li><li><strong>Reliable and durable:</strong> Six replicas of your data across three Availability Zones with full backup and restore.&nbsp;</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Redshift\n",
    "<div class=\"fr-view\"><p>General characteristics:&nbsp;</p><ul><li><strong>Fast:</strong> Columnar storage technology to improve I/O efficiency and parallelize queries. Data loads linearly. Redshift also provides fastest query results using higher cost storage.</li><li><strong>Reliable and durable:</strong> Replicates your data within your data warehouse cluster and continuously backs up your data to Amazon S3, which is designed for eleven nines of durability.</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon S3\n",
    "<div class=\"fr-view\"><p>General characteristics:&nbsp;</p><ul><li><strong>Fast:</strong> Amazon S3 storage provides a fast reliable platform for querying structured and semistructured data. Redshift spectrum and Amazon Athena are able to query data without moving it from Amazon S3 at high speed with low latency.</li><li><strong>Reliable and durable:</strong> Data is always replicated across three Availability Zones in the same region. Amazon S3 also offers same-region and &nbsp;cross-region replication for greater durability.&nbsp;</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "\ttable.tableizer-table {\n",
    "\t\tfont-size: 12px;\n",
    "\t\tborder: 1px solid #CCC; \n",
    "\t\tfont-family: Arial, Helvetica, sans-serif;\n",
    "\t} \n",
    "\t.tableizer-table td {\n",
    "\t\tpadding: 4px;\n",
    "\t\tmargin: 3px;\n",
    "\t\tborder: 1px solid #CCC;\n",
    "\t}\n",
    "\t.tableizer-table th {\n",
    "\t\tbackground-color: #104E8B; \n",
    "\t\tcolor: #FFF;\n",
    "\t\tfont-weight: bold;\n",
    "\t}\n",
    "</style>\n",
    "<table class=\"tableizer-table\">\n",
    "<thead><tr class=\"tableizer-firstrow\"><th></th><th>Amazon ElastiCache</th><th>Amazon DynamoDB + Amazon DynamoDB Accelerator (DAX)</th><th>Amazon Aurora</th><th>Amazon RDS </th><th>Amazon Neptune</th><th>Amazon S3 + Amazon S3 Glacier </th><th>Amazon Redshift</th></tr></thead><tbody>\n",
    " <tr><td>Use cases</td><td>In-memory caching</td><td>K/V lookups, document store</td><td>OLTP, transactional</td><td>OLTP, transactional</td><td>Graph</td><td>File Store</td><td>cloud data warehouse</td></tr>\n",
    " <tr><td>Throughput</td><td>Ultra-high request rate, Ultra-low latency</td><td>Ultra-high request rate, Ultra-low to low latency </td><td>Very high request rate, low latency</td><td>High request rate, low latency</td><td>Medium request rate, low latency</td><td>High throughput</td><td>High request rate, low latency</td></tr>\n",
    " <tr><td>Shape</td><td>K/V</td><td>K/V and document</td><td>Relational</td><td>Relational</td><td>Node/edges</td><td>Files</td><td>Nodes/Columnar Storage</td></tr>\n",
    " <tr><td>Size</td><td>GB</td><td>TB, PB (no limits)</td><td>GB, mid TB</td><td>GB, low TB</td><td>GB, mid TB</td><td>GB, TB, PB, EB (no limits)</td><td>TB, PB (no limits)</td></tr>\n",
    " <tr><td>Cost/GB</td><td><span>&#36;</span><span>&#36;</span></td> <td><span>&#162;</span><span>&#162;</span> - <span>&#36;</span><span>&#36;</span></td> <td><span>&#162;</span><span>&#162;</span></td><td><span>&#162;</span><span>&#162;</span></td><td><span>&#162;</span><span>&#162;</span></td><td><span>&#162;</span></td><td><span>&#162;</span><span>&#162;</span> - <span>&#36;</span><span>&#36;</span></td></tr>\n",
    " <tr><td>Availability</td><td>2 AZ</td><td>3 AZ</td><td>3 AZ</td><td>2 AZ</td><td>3 AZ</td><td>3 AZ</td><td>Currently, Amazon Redshift only supports Single-AZ deployments. Backup is replicated to S3 (3 AZ)</td></tr>\n",
    " <tr><td>Amazon Virtual Private Cloud (VPC)support</td><td>Inside VPC</td><td>DynamoDB has a VPC endpoint and DAX is inside VPC</td><td>Inside VPC</td><td>Inside VPC</td><td>Inside VPC</td><td>VPC endpoint</td><td>inside VPC</td></tr>\n",
    "<tr><td>Data Storage Type</td><td>Operational</td><td>Operational</td><td>Operational</td><td>Operational</td><td>Operational</td><td>Analytic</td><td>Analytic</td></tr>\n",
    "<tr><td>How long do you need to store the data</td><td>Storing transient data</td><td>Storing long-term data(DynamoDB) / Storing transient data(DAX)</td><td>Storing long-term data</td><td>Storing long-term data</td><td>Storing long-term data</td><td>Storing long-term data(S3) / Storing archived data(Glacier) </td><td>Storing long-term data</td></tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What type of data do you need to store?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%;\"><thead><tr><th style=\"width:26.6549%;background-color:rgb(13, 78, 70);\"><span style=\"color:rgb(255, 255, 255);\"><strong>Attribute</strong></span></th><th style=\"width:39.968%;background-color:rgb(13, 78, 70);\"><span style=\"color:rgb(255, 255, 255);font-weight:bold;\">Data Warehouse</span><br></th><th style=\"width:33.3333%;background-color:rgb(13, 78, 70);\"><span style=\"color:rgb(255, 255, 255);font-weight:bold;\">Data Lake</span></th></tr></thead><tbody><tr><td style=\"text-align:center;width:26.6549%;background-color:rgb(239, 239, 239);\"><span style=\"color:rgb(49, 53, 55);\"><strong>Data</strong></span></td><td style=\"text-align:center;width:39.968%;\"><strong>Relational from transactional systems, operational databases, and line of business applications</strong></td><td style=\"text-align:center;width:33.3333%;\"><strong>Nonrelational and relational from IoT devices, web sites, mobile apps, social media, and corporate applications</strong></td></tr><tr><td style=\"text-align:center;width:26.6549%;background-color:rgb(239, 239, 239);\"><strong><span style=\"color:rgb(49, 53, 55);\">Schema</span></strong><span style=\"color:rgb(255, 255, 255);\"><br></span></td><td style=\"text-align:center;width:39.968%;\"><strong>Designed prior to the implementation of the data warehouse (schema-on-write)</strong><br></td><td style=\"text-align:center;width:33.3333%;\"><strong>Written at the time of analysis (schema-on-read)</strong><br></td></tr><tr><td style=\"text-align:center;width:26.6549%;background-color:rgb(239, 239, 239);\"><span style=\"color:rgb(49, 53, 55);\"><strong>Price/performance</strong></span><span style=\"color:rgb(255, 255, 255);\"><strong><br></strong></span></td><td style=\"text-align:center;width:39.968%;\"><strong>Fastest query results using higher cost storage</strong></td><td style=\"text-align:center;width:33.3333%;\"><strong>Query results getting faster using low-cost storage</strong></td></tr><tr><td style=\"text-align:center;width:26.6549%;background-color:rgb(239, 239, 239);\"><strong><span style=\"color:rgb(49, 53, 55);\">Data quality</span></strong><span style=\"color:rgb(255, 255, 255);\"><br></span></td><td style=\"text-align:center;width:39.968%;\"><strong>Highly curated data,</strong><br><strong>serves as the central version of the truth</strong><br></td><td style=\"text-align:center;width:33.3333%;\"><strong>Raw data,</strong><br><strong>may or may not be curated</strong><br></td></tr><tr><td style=\"text-align:center;width:26.6549%;background-color:rgb(239, 239, 239);\"><strong><span style=\"color:rgb(49, 53, 55);\">Analytics</span></strong><span style=\"color:rgb(255, 255, 255);\"><br></span></td><td style=\"text-align:center;width:39.968%;\"><strong>Batch reporting, ad-hoc analysis, business intelligence, and data visualization</strong><br></td><td style=\"text-align:center;width:33.3333%;\"><strong>Machine learning, predictive analytics, data discovery, and profiling</strong><br></td></tr></tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizing Amazon Redshift "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Redshift is a massively parallel processing (MPP) system. Each Amazon Redshift cluster has one leader node, two or more compute nodes, and each compute node has a number of slices. Each node slice is an independent partition of data. The slices can perform operations in parallel to complete operations significantly faster than systems that do not use slices. Because of the massive amount of data involved in data warehousing, you must design your database to take full advantage of every performance optimization. The four types of Amazon Redshift optimizations that this section will cover are data distribution styles, sort key best practices, compression encoding implementations, and data size optimizations.\n",
    "##### Amazon Redshift data redistribution\n",
    "\n",
    "When you load data into a table, Amazon Redshift distributes the table's rows to the compute nodes and slices according to the distribution style that you chose when you created the table. The distribution strategy that you choose for your database has important consequences for query performance, storage requirements, data loading, and maintenance. By choosing the best distribution style for each table, you can balance your data distribution and significantly improve overall system performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KEY distribution\n",
    "<p>The rows are distributed according to the values in one column. The leader node attempts to place matching values on the same node slice. If you distribute a pair of tables on the joining keys, the leader node collocates the rows on the slices according to the values in the joining columns so that matching values from the common columns are physically stored together.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVEN distribution\n",
    "<p>The leader node distributes the rows across the slices in a round-robin fashion(rows are always evenly distributed across slices), regardless of the values in any particular column. EVEN distribution is appropriate when a table does not participate in joins or when there is not a clear choice between KEY distribution and ALL distribution.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALL distribution\n",
    "<p>A copy of the entire table is distributed to every node. Where EVEN distribution or KEY distribution place only a portion of a table's rows on each node, ALL distribution ensures that every row is collocated for every join that the table participates in. &nbsp;ALL distribution multiplies the storage required by the number of nodes in the cluster, and so it takes much longer to load, update, or insert data into multiple tables. ALL distribution is appropriate only for relatively slow-moving tables; that is, tables that are not updated frequently or extensively. Small dimension tables do not benefit significantly from ALL distribution, because the cost of redistribution is low.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUTO distribution\n",
    "<p>With AUTO distribution, Amazon Redshift assigns an optimal distribution style based on the size of the table data. For example, Amazon Redshift initially assigns ALL distribution to a small table, then changes to EVEN distribution when the table grows larger. When a table is changed from ALL to EVEN distribution, storage utilization might change slightly. The change in distribution occurs in the background, in a few seconds. Amazon Redshift never changes the distribution style from EVEN to ALL. If you don't specify a distribution style with the CREATE TABLE statement, Amazon Redshift applies AUTO distribution.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choosing a sort key in Amazon Redshift  \n",
    "\n",
    "When you create a table, you can define one or more of its columns as sort keys. When data is initially loaded into the empty table, the rows are stored on disk in sorted order. Information about sort key columns is passed to the query planner, and the planner uses this information to construct plans that exploit the way that the data is sorted. You can specify either a compound or interleaved sort key.\n",
    "<a href=\"https://docs.aws.amazon.com/redshift/latest/dg/t_Sorting_data.html\">Choosing sort keys</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interleaved\n",
    "<p>An interleaved sort key gives equal weight to each column in the sort key, so query predicates can use any subset of the columns that make up the sort key, in any order. The performance improvements you gain by implementing an interleaved sort key should be weighed against increased load and vacuum times. Donâ€™t use an interleaved sort key on columns with monotonically increasing attributes, such as identity columns, dates, or timestamps.</p>\n",
    "An interleaved sort gives equal weight to each column, or subset of columns, in the sort key. If multiple queries use different columns for filters, then you can often improve performance for those queries by using an interleaved sort style. When a query uses restrictive predicates on secondary sort columns, interleaved sorting significantly improves query performance as compared to compound sorting.\n",
    "Important\n",
    "\n",
    "Don't use an interleaved sort key on columns with monotonically increasing attributes, such as identity columns, dates, or timestamps.\n",
    "\n",
    "The performance improvements you gain by implementing an interleaved sort key should be weighed against increased load and vacuum times.\n",
    "\n",
    "Interleaved sorts are most effective with highly selective queries that filter on one or more of the sort key columns in the WHERE clause, for example select c_name from customer where c_region = 'ASIA'. The benefits of interleaved sorting increase with the number of sorted columns that are restricted.\n",
    "\n",
    "An interleaved sort is more effective with large tables. Sorting is applied on each slice, so an interleaved sort is most effective when a table is large enough to require multiple 1 MB blocks per slice and the query processor is able to skip a significant proportion of the blocks using restrictive predicates. To view the number of blocks a table uses, query the STV_BLOCKLIST system view.\n",
    "\n",
    "When sorting on a single column, an interleaved sort might give better performance than a compound sort if the column values have a long common prefix. For example, URLs commonly begin with \"http://www\". Compound sort keys use a limited number of characters from the prefix, which results in a lot of duplication of keys. Interleaved sorts use an internal compression scheme for zone map values that enables them to better discriminate among column values that have a long common prefix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compound\n",
    "<p>A compound sort key is more efficient when query predicates use a prefix, which is a subset of the sort key columns in order.&nbsp;<br>Compound sort keys might speed up joins, GROUP BY and ORDER BY operations, and window functions that use PARTITION BY and ORDER BY. For example, a merge join, which is often faster than a hash join, is feasible when the data is distributed and presorted on the joining columns. Compound sort keys also help improve compression.<br>The performance benefits of compound sorting decrease when queries depend only on secondary sort columns, without referencing the primary columns. COMPOUND is the default sort type.</p>\n",
    " A compound key is made up of all of the columns listed in the sort key definition, in the order they are listed. A compound sort key is most useful when a query's filter applies conditions, such as filters and joins, that use a prefix of the sort keys. The performance benefits of compound sorting decrease when queries depend only on secondary sort columns, without referencing the primary columns. COMPOUND is the default sort type.\n",
    "\n",
    "Compound sort keys might speed up joins, GROUP BY and ORDER BY operations, and window functions that use PARTITION BY and ORDER BY. For example, a merge join, which is often faster than a hash join, is feasible when the data is distributed and presorted on the joining columns. Compound sort keys also help improve compression.\n",
    "\n",
    "As you add rows to a sorted table that already contains data, the unsorted region grows, which has a significant effect on performance. The effect is greater when the table uses interleaved sorting, especially when the sort columns include data that increases monotonically, such as date or timestamp columns. You should run a VACUUM operation regularly, especially after large data loads, to re-sort and re-analyze the data. For more information, see Managing the size of the unsorted region. After vacuuming to resort the data, it's a good practice to run an ANALYZE command to update the statistical metadata for the query planner. For more information, see Analyzing tables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Vacuuming tables</h5>\n",
    "Amazon Redshift can automatically sort and perform a VACUUM DELETE operation on tables in the background. To clean up tables after a load or a series of incremental updates, you can also run the VACUUM command, either against the entire database or against individual tables. \n",
    "\n",
    "<h6>Automatic table sort</h6>\n",
    "\n",
    "Amazon Redshift automatically sorts data in the background to maintain table data in the order of its sort key. Amazon Redshift keeps track of your scan queries to determine which sections of the table will benefit from sorting.\n",
    "\n",
    "Depending on the load on the system, Amazon Redshift automatically initiates the sort. This automatic sort lessens the need to run the VACUUM command to keep data in sort key order. If you need data fully sorted in sort key order, for example after a large data load, then you can still manually run the VACUUM command. To determine whether your table will benefit by running VACUUM SORT, monitor the vacuum_sort_benefit column in SVV_TABLE_INFO.\n",
    "\n",
    "Amazon Redshift tracks scan queries that use the sort key on each table. Amazon Redshift estimates the maximum percentage of improvement in scanning and filtering of data for each table (if the table was fully sorted). This estimate is visible in the vacuum_sort_benefit column in SVV_TABLE_INFO. You can use this column, along with the unsorted column, to determine when queries can benefit from manually running VACUUM SORT on a table. The unsorted column reflects the physical sort order of a table. The vacuum_sort_benefit column specifies the impact of sorting a table by manually running VACUUM SORT. \n",
    "\n",
    "<h6>Automatic vacuum delete</h6>\n",
    "\n",
    "When you perform a delete, the rows are marked for deletion, but not removed. Amazon Redshift automatically runs a VACUUM DELETE operation in the background based on the number of deleted rows in database tables. Amazon Redshift schedules the VACUUM DELETE to run during periods of reduced load and pauses the operation during periods of high load. \n",
    "\n",
    "<h6>VACUUM frequency</h6>\n",
    "\n",
    "You should vacuum as often as you need to in order to maintain consistent query performance. Consider these factors when determining how often to run your VACUUM command. \n",
    "<ul>\n",
    "    <li>Run VACUUM during time periods when you expect minimal activity on the cluster, such as evenings or during designated database administration windows.</li>\n",
    "    <li>A large unsorted region results in longer vacuum times. If you delay vacuuming, the vacuum will take longer because more data has to be reorganized.</li>\n",
    "    <li>VACUUM is an I/O intensive operation, so the longer it takes for your vacuum to complete, the more impact it will have on concurrent queries and other database operations running on your cluster.</li>\n",
    "    <li>VACUUM takes longer for tables that use interleaved sorting. To evaluate whether interleaved tables need to be re-sorted, query the SVV_INTERLEAVED_COLUMNS view.</li>\n",
    "<ul>\n",
    "      \n",
    "<h6>Sort stage and merge stage</h6>\n",
    "\n",
    "Amazon Redshift performs a vacuum operation in two stages: first, it sorts the rows in the unsorted region, then, if necessary, it merges the newly sorted rows at the end of the table with the existing rows. When vacuuming a large table, the vacuum operation proceeds in a series of steps consisting of incremental sorts followed by merges. If the operation fails or if Amazon Redshift goes off line during the vacuum, the partially vacuumed table or database will be in a consistent state, but you will need to manually restart the vacuum operation. Incremental sorts are lost, but merged rows that were committed before the failure do not need to be vacuumed again. If the unsorted region is large, the lost time might be significant. For more information about the sort and merge stages, see Managing the volume of merged rows.\n",
    "\n",
    "Users can access tables while they are being vacuumed. You can perform queries and write operations while a table is being vacuumed, but when DML and a vacuum run concurrently, both might take longer. If you execute UPDATE and DELETE statements during a vacuum, system performance might be reduced. Incremental merges temporarily block concurrent UPDATE and DELETE operations, and UPDATE and DELETE operations in turn temporarily block incremental merge steps on the affected tables. DDL operations, such as ALTER TABLE, are blocked until the vacuum operation finishes with the table. \n",
    "\n",
    "<h6>Vacuum threshold</h6>\n",
    "\n",
    "By default, VACUUM skips the sort phase for any table where more than 95 percent of the table's rows are already sorted. Skipping the sort phase can significantly improve VACUUM performance. To change the default sort threshold for a single table, include the table name and the TO threshold PERCENT parameter when you run the VACUUM command. \n",
    "\n",
    "<h6>Vacuum types</h6>\n",
    "\n",
    "You can run a full vacuum, a delete only vacuum, a sort only vacuum, or a reindex with full vacuum.\n",
    "<ul>\n",
    "    <li>VACUUM FULL re-sorts rows and reclaims space from deleted rows. Amazon Redshift automatically performs VACUUM DELETE ONLY operations in the background, so for most applications, VACUUM FULL and VACUUM SORT ONLY are equivalent. VACUUM FULL is the same as VACUUM. Full vacuum is the default vacuum operation.</li>\n",
    "    <li>A DELETE ONLY vacuum is the same as a full vacuum except that it skips the sort. Amazon Redshift automatically performs a DELETE ONLY vacuum in the background, so you rarely, if ever, need to run a DELETE ONLY vacuum.\n",
    "    </li>\n",
    "    <li>VACUUM SORT ONLY doesn't reclaim disk space. In most cases there is little benefit compared to a full vacuum.</li>\n",
    "    <li>VACUUM REINDEX for tables that use interleaved sort keys. For more information about interleaved sort keys, see Interleaved sort key.</li>\n",
    "</ul>\n",
    "<p>When you initially load an empty interleaved table using COPY or CREATE TABLE AS, Amazon Redshift automatically builds the interleaved index. If you initially load an interleaved table using INSERT, you need to run VACUUM REINDEX afterwards to initialize the interleaved index. REINDEX reanalyzes the distribution of the values in the table's sort key columns, then performs a full VACUUM operation. VACUUM REINDEX takes significantly longer than VACUUM FULL because it needs to take an extra analysis pass over the data, and because merging in new interleaved data can involve touching all the data blocks. If a VACUUM REINDEX operation terminates before it completes, the next VACUUM resumes the reindex operation before performing the vacuum.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Evaluating the query plan</h5>\n",
    "You can use query plans to identify candidates for optimizing the distribution style. After making your initial design decisions, create your tables, load them with data, and test them. Use a test dataset that is as close as possible to the real data. Measure load times to use as a baseline for comparisons. Evaluate queries that are representative of the most costly queries you expect to execute; specifically, queries that use joins and aggregations. Compare execution times for various design options. When you compare execution times, do not count the first time the query is executed, because the first run time includes the compilation time.\n",
    "\n",
    "DS_DIST_NONE\n",
    "\n",
    "    No redistribution is required, because corresponding slices are collocated on the compute nodes. You will typically have only one DS_DIST_NONE step, the join between the fact table and one dimension table.\n",
    "DS_DIST_ALL_NONE\n",
    "\n",
    "    No redistribution is required, because the inner join table used DISTSTYLE ALL. The entire table is located on every node.\n",
    "DS_DIST_INNER\n",
    "\n",
    "    The inner table is redistributed.\n",
    "DS_DIST_OUTER\n",
    "\n",
    "    The outer table is redistributed.\n",
    "DS_BCAST_INNER\n",
    "\n",
    "    A copy of the entire inner table is broadcast to all the compute nodes.\n",
    "DS_DIST_ALL_INNER\n",
    "\n",
    "    The entire inner table is redistributed to a single slice because the outer table uses DISTSTYLE ALL.\n",
    "DS_DIST_BOTH\n",
    "\n",
    "    Both tables are redistributed.\n",
    "\n",
    "DS_DIST_NONE and DS_DIST_ALL_NONE are good. They indicate that no distribution was required for that step because all of the joins are collocated.\n",
    "\n",
    "DS_DIST_INNER means that the step will probably have a relatively high cost because the inner table is being redistributed to the nodes. DS_DIST_INNER indicates that the outer table is already properly distributed on the join key. Set the inner table's distribution key to the join key to convert this to DS_DIST_NONE. If distributing the inner table on the join key is not possible because the outer table is not distributed on the join key, evaluate whether to use ALL distribution for the inner table. If the table is relatively slow moving, that is, it is not updated frequently or extensively, and it is large enough to carry a high redistribution cost, change the distribution style to ALL and test again. ALL distribution causes increased load times, so when you retest, include the load time in your evaluation factors.\n",
    "\n",
    "DS_DIST_ALL_INNER is not good. It means the entire inner table is redistributed to a single slice because the outer table uses DISTSTYLE ALL, so that a copy of the entire outer table is located on each node. This results in inefficient serial execution of the join on a single node instead taking advantage of parallel execution using all of the nodes. DISTSTYLE ALL is meant to be used only for the inner join table. Instead, specify a distribution key or use even distribution for the outer table.\n",
    "\n",
    "DS_BCAST_INNER and DS_DIST_BOTH are not good. Usually these redistributions occur because the tables are not joined on their distribution keys. If the fact table does not already have a distribution key, specify the joining column as the distribution key for both tables. If the fact table already has a distribution key on another column, you should evaluate whether changing the distribution key to collocate this join will improve overall performance. If changing the distribution key of the outer table is not an optimal choice, you can achieve collocation by specifying DISTSTYLE ALL for the inner table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Workload Management (WLM)</h5>\n",
    "<a href=\"https://docs.aws.amazon.com/redshift/latest/dg/cm-c-implementing-workload-management.html\">Implementing workload management</a> You can use workload management (WLM) to define multiple query queues and to route queries to the appropriate queues at runtime. In some cases, you might have multiple sessions or users running queries at the same time. In these cases, some queries might consume cluster resources for long periods of time and affect the performance of other queries. For example, suppose that one group of users submits occasional complex, long-running queries that select and sort rows from several large tables. Another group frequently submits short queries that select only a few rows from one or two tables and run in a few seconds. In this situation, the short-running queries might have to wait in a queue for a long-running query to complete. WLM helps manage this situation. You can configure Amazon Redshift WLM to run with either automatic WLM or manual WLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compression encodings\n",
    "<div class=\"fr-view\"><p>A compression encoding specifies the type of compression that is applied to a column of data values as rows are added to a table. Learn more about the different compression types: </p><ul><li><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_Raw_encoding.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">Raw Encoding:</a>Raw encoding is the default encoding for columns that are designated as sort keys and columns that are defined as BOOLEAN, REAL, or DOUBLE PRECISION data types. With raw encoding, data is stored in raw, uncompressed form. </li><li><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/az64-encoding.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">AZ64 Encoding:</a>AZ64 is Amazon's proprietary compression encoding algorithm designed to achieve a high compression ratio and improved query processing. At its core, the AZ64 algorithm compresses smaller groups of data values and uses single instruction, multiple data (SIMD) instructions for parallel processing. Use AZ64 to achieve significant storage savings and high performance for numeric, date, and time data types.</li><li><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_Byte_dictionary_encoding.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">Byte-Dictionary Encoding</a></li><li><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_Delta_encoding.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">Delta Encoding</a></li><li><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/lzo-encoding.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">LZO Encoding:</a> provides a very high compression ratio with good performance. LZO encoding works especially well for CHAR and VARCHAR columns that store very long character strings, especially free form text, such as product descriptions, user comments, or JSON strings.</li><li><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_MostlyN_encoding.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">Mostly Encoding</a></li><li><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_Runlength_encoding.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">Runlength Encoding</a></li><li><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_Text255_encoding.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">Text255 and Text32k Encodings</a></li><li><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/zstd-encoding.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">Zstandard Encoding</a></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"fr-view\"><p>If no compression is specified in a CREATE TABLE or ALTER TABLE statement, Amazon Redshift automatically assigns compression encoding as follows:</p><ul><li><p>Columns that are defined as BOOLEAN, REAL, or DOUBLE PRECISION data types are assigned RAW compression.</p></li><li><p>Columns that are defined as SMALLINT, INTEGER, BIGINT, DECIMAL, DATE, TIMESTAMP, or TIMESTAMPTZ data types are assigned AZ64 compression.</p></li><li><p>Columns that are defined as CHAR or VARCHAR data types are assigned LZO compression.</p></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizing size of data in Amazon Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"fr-view\"><p>Best practices for optimizing data size:&nbsp;</p><ul><li>Split your load data files so that the files are about equal size, between 1 MB and 1 GB after compression. For optimum parallelism, the ideal size is between 1 MB and 125 MB after compression.&nbsp;</li><li>The number of files should be a multiple of the number of slices in your cluster.</li><li>When loading data, we strongly recommend that you individually compress your load files using gzip, lzop, bzip2, or Zstandard when you have large datasets. See more Amazon Redshift loading best practices <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-compress-data-files.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">here</a>.</li><li>You can also apply compression encodings when creating a table, as discussed previously in the module.</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing (24 %)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMR data-encryption-options\n",
    "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-data-encryption-options.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Configure Instance Fleets</h5>\n",
    "The instance fleets configuration for a cluster offers the widest variety of provisioning options for EC2 instances. With instance fleets, you specify target capacities for On-Demand Instances and Spot Instances within each fleet. When the cluster launches, Amazon EMR provisions instances until the targets are fulfilled. You can specify up to five EC2 instance types per fleet for Amazon EMR to use when fulfilling the targets. You can also select multiple subnets for different Availability Zones. When Amazon EMR launches the cluster, it looks across those subnets to find the instances and purchasing options you specify.\n",
    "\n",
    "While a cluster is running, if Amazon EC2 reclaims a Spot Instance because of a price increase, or an instance fails, Amazon EMR tries to replace the instance with any of the instance types that you specify. This makes it easier to regain capacity during a spike in Spot pricing. Instance fleets allow you to develop a flexible and elastic resourcing strategy for each node type. For example, within specific fleets, you can have a core of On-Demand capacity supplemented with less-expensive Spot capacity if available, and then switch to On-Demand capacity if Spot isn't available at your price. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Apache Hue</h5>\n",
    "Hue is an open source SQL Assistant for Databases & Data Warehouses\n",
    "\n",
    "<h5>Apache Tez</h5>\n",
    "The project is aimed at building an application framework which allows for a complex directed-acyclic-graph of tasks for processing data. It is currently built atop Apache Hadoop YARN.\n",
    "\n",
    "The 2 main design themes for Tez are:\n",
    "\n",
    "    Empowering end users by:\n",
    "        Expressive dataflow definition APIs\n",
    "        Flexible Input-Processor-Output runtime model\n",
    "        Data type agnostic\n",
    "        Simplifying deployment\n",
    "    Execution Performance\n",
    "        Performance gains over Map Reduce\n",
    "        Optimal resource management\n",
    "        Plan reconfiguration at runtime\n",
    "        Dynamic physical data flow decisions\n",
    "\n",
    "By allowing projects like Apache Hive and Apache Pig to run a complex DAG of tasks, Tez can be used to process data, that earlier took multiple MR jobs, now in a single Tez job.\n",
    "\n",
    "<h5>Ganglia</h5>\n",
    "Ganglia is a scalable distributed monitoring system for high-performance computing systems such as clusters and Grids. It is based on a hierarchical design targeted at federations of clusters. It leverages widely used technologies such as XML for data representation, XDR for compact, portable data transport, and RRDtool for data storage and visualization.\n",
    "\n",
    "<h5>Apache Sqoop</h5>\n",
    "Apache Sqoop(TM) is a tool designed for efficiently transferring bulk data between Apache Hadoop and structured datastores such as relational databases.\n",
    "\n",
    "<h5>Apache Oozie</h5>\n",
    "Oozie is a workflow scheduler system to manage Apache Hadoop jobs. Oozie Workflow jobs are Directed Acyclical Graphs (DAGs) of actions. Oozie Coordinator jobs are recurrent Oozie Workflow jobs triggered by time (frequency) and data availability. Oozie is integrated with the rest of the Hadoop stack supporting several types of Hadoop jobs out of the box (such as Java map-reduce, Streaming map-reduce, Pig, Hive, Sqoop and Distcp) as well as system specific jobs (such as Java programs and shell scripts). Oozie is a scalable, reliable and extensible system.\n",
    "\n",
    "<h5>Jupyter Notebook</h5>\n",
    "The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modeling, data manipulation and so on.\n",
    "\n",
    "<h5>Presto</h5>\n",
    "Presto is an open source distributed SQL query engine for running interactive analytic queries against data sources of all sizes ranging from gigabytes to petabytes.\n",
    "\n",
    "<h5>Apache Hive</h5>\n",
    "Apache Hive is for providing data query and analysis which is a data warehouse software project, built on top of Apache Hadoop. Hive gives an SQL-like interface to query data stored in various databases and file systems that integrate with Hadoop. Traditional SQL queries must be implemented in the MapReduce Java API to execute SQL applications and queries over distributed data. Hive provides the necessary SQL abstraction to integrate SQL-like queries (HiveQL) into the underlying Java without the need to implement queries in the low-level Java API.\n",
    "\n",
    "<h5>Apache HBase</h5>\n",
    "Apache HBase is the Hadoop database, a distributed, scalable, big data store.\n",
    "\n",
    "<h5>Apache Phoenix</h5>\n",
    "Apache Phoenix enables OLTP and operational analytics in Hadoop for low latency applications by combining the best of both worlds:\n",
    "\n",
    "    the power of standard SQL and JDBC APIs with full ACID transaction capabilities and\n",
    "    the flexibility of late-bound, schema-on-read capabilities from the NoSQL world by leveraging HBase as its backing store\n",
    "\n",
    "Apache Phoenix is fully integrated with other Hadoop products such as Spark, Hive, Pig, Flume, and Map Reduce.\n",
    "\n",
    "<h5>Apache Flink</h5>\n",
    "Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.\n",
    "\n",
    "<h5>Apache Zeppelin</h5>\n",
    "Web-based notebook that enables data-driven, interactive data analytics and collaborative documents with SQL, Scala and more.\n",
    "\n",
    "<h5>Apache Livy</h5>\n",
    "REST on Spark, Livy enables programmatic, fault-tolerant, multi-tenant submission of Spark jobs from web/mobile apps (no Spark client needed). So, multiple users can interact with your Spark cluster concurrently and reliably.\n",
    "\n",
    "<h5>Apache Zookeeper</h5>\n",
    "ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. All of these kinds of services are used in some form or another by distributed applications.\n",
    "\n",
    "<h5>Apache Mahout</h5>\n",
    "For Creating Scalable Performant Machine Learning Applications\n",
    "\n",
    "<h5>HCatalog</h5>\n",
    "HCatalog is a table storage management tool for Hadoop that exposes the tabular data of Hive metastore to other Hadoop applications. It enables users with different data processing tools (Pig, MapReduce) to easily write data onto a grid. HCatalog ensures that users donâ€™t have to worry about where or in what format their data is stored.\n",
    "\n",
    "<h5>Apache Pig</h5>\n",
    "Apache Pig is a platform for analyzing large data sets that consists of a high-level language for expressing data analysis programs, coupled with infrastructure for evaluating these programs. The salient property of Pig programs is that their structure is amenable to substantial parallelization, which in turns enables them to handle very large data sets.\n",
    "\n",
    "<h5>Apache Mxnet</h5>\n",
    "open source deep learning framework suited for flexible research prototyping and production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Visualization (18%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://aws.amazon.com/big-data/datalakes-and-analytics/\">Data Lakes and Analytics on AWS</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "\ttable.tableizer-table {\n",
    "\t\tfont-size: 12px;\n",
    "\t\tborder: 1px solid #CCC; \n",
    "\t\tfont-family: Arial, Helvetica, sans-serif;\n",
    "\t} \n",
    "\t.tableizer-table td {\n",
    "\t\tpadding: 4px;\n",
    "\t\tmargin: 3px;\n",
    "\t\tborder: 1px solid #CCC;\n",
    "\t}\n",
    "\t.tableizer-table th {\n",
    "\t\tbackground-color: #104E8B; \n",
    "\t\tcolor: #FFF;\n",
    "\t\tfont-weight: bold;\n",
    "\t}\n",
    "</style>\n",
    "<table class=\"tableizer-table\">\n",
    "<thead><tr class=\"tableizer-firstrow\"><th></th><th>Cost</th><th>Performance</th><th>Durability and availability</th><th>Scalability and elasticity</th><th>Interfaces</th></tr></thead><tbody>\n",
    " <tr><td>Amazon Athena</td><td>Pay for the resources you consume. Priced per query, per TB of data scanned, and charges based on the amount of data scanned by the query. </td><td>You can improve the performance of your query by compressing, partitioning, and converting your data into columnar formats.</td><td>Amazon Athena is highly available and executes queries using compute resources across multiple facilities, automatically routing queries appropriately if a particular facility is unreachable. </td><td>Athena is serverless, so there is no infrastructure to set up or manage, and it can scale automatically, as needed.</td><td>Athena Console and CLI, API via SDK and JDBC. Athena integrates with Amazon QuickSight. Athena that are registered with the AWS Glue Data Catalog.</td></tr>\n",
    " <tr><td>Amazon ES</td><td>Pay only for what you use. You are charged for Amazon ES instance hours, Amazon EBS storage (if you choose this option), and standard data transfer fees.</td><td>Performance depends on multiple factors including: â€¢instance type â€¢workload â€¢index â€¢number of shards used â€¢read replica configuration â€¢storage configuration (instance or EBS). Indexes are made up of shards of data which can be distributed on different instances in multiple Availability Zones.</td><td>Enable zone awareness for high availability. When Zone Awareness is enabled, Amazon ES distributes the instances supporting the domain across two different Availability Zones. Then, if you enable replicas in Amazon ES, the instances are automatically distributed to deliver cross-zone replication.  </td><td>You can add or remove instances, and modify Amazon EBS volumes to accommodate data growth.</td><td>Amazon ES supports integration with AWS services for streaming data. Some sources have built-in support for Amazon ES, others use Lambda functions as event handlers.</td></tr>\n",
    " <tr><td>Amazon EMR</td><td>you only pay for the hours the cluster is up. You can launch a persistent cluster that stays up indefinitely or a temporary cluster that terminates after the analysis is complete.</td><td>Amazon EMR performance is driven by the type and number of EC2 instances you choose to run your analytics. Consider processing requirements, sufficient memory, storage, and processing power.</td><td>By default, Amazon EMR is fault tolerant for core node failures and continues job execution if a slave node goes down. When a core node fails, Amazon EMR will provision a new node. If all nodes in the cluster are lost, Amazon EMR will not replace them. </td><td>You can resize your cluster to add instances for peak workloads and remove instances to control costs when peak workloads subside</td><td>Amazon EMR supports many tools on top of Hadoop that can be used for big data analytics and each has its own interfaces.</td></tr>\n",
    " <tr><td>Amazon Kinesis Data Stream</td><td>â€¢There are just two pricing components, an hourly charge per shard and a charge for each 1 million PUT transactions. â€¢The use of enhanced fan-out configuration or extended retention periods has additional costs.</td><td>Choose throughput capacity in terms of shards. The enhanced fan-out option can improve performance by increasing the throughput available to each individual consumer.</td><td>Kinesis Data Streams synchronously replicates data across three Availability Zones in an AWS Region, and stores that data for up to seven days.</td><td>The initial scale is based on the number of shards you select for the stream. You can increase or decrease the capacity of the stream at any time. Use API calls or development tools to automate scaling.</td><td>Producers: Amazon Kinesis PUT API, SDK or toolkit abstraction, the Amazon Kinesis Producer Library (KPL), or the Amazon Kinesis Agent. Consumer:KCL, Kinesis Data Analytics, Kinesis Data Firehose, and AWS Lambda.</td></tr>\n",
    " <tr><td>Amazon Kinesis Data Firehose</td><td>Volume of data ingested (the number of data records you send to the service) *( the size of each record rounded up to the nearest 5KB). format conversion like Parquet, ORC has charges</td><td>Specify a batch size or batch interval and data compression to control how quickly data is uploaded to destinations</td><td>All Kinesis services are fully managed.</td><td>Streams automatically scale up and down based on the data rate you specify for the stream.</td><td>Kinesis data stream, the Kinesis Agent, or the Kinesis Data Firehose API using the AWS SDK to write to a Kinesis Data Firehose stream. Amazon CloudWatch Logs, CloudWatch Events, or AWS IoT Core as your data source. Kinesis Data Firehose streams can deliver data to one of four destinations: Amazon S3, Amazon ES, Amazon Redshift, or Splunk.</td></tr>\n",
    " <tr><td>Amazon Data Analytics</td><td>Kinesis Processing Units (or KPUs) by hour used to run your stream processing application. 1 KPU is 1 vCPU compute and 4 GB memory. For Java applications, you are charged a single additional KPU per application for application orchestration</td><td>Amazon Kinesis Data Analytics elastically scales your application to accommodate the data throughput of your source stream and your query complexity for most scenarios. Kinesis Data Analytics provisions capacity in the form of Kinesis Processing Units (KPU).</td><td>For Kinesis Data Analytics applications, you can create and delete durable application backups through a simple API call.</td><td>Set up your application for your future scaling needs by proactively increasing the number of input in-application streams from the default (one). Use multiple streams and Kinesis Data Analytics for SQL applications if your application has scaling needs beyond 100 MB/second.                               Use Kinesis Data Analytics for Java Applications if you want to use a single stream and application.  </td><td>Kinesis data stream or a Kinesis Data Firehose delivery stream.external destination such as an Amazon Kinesis data stream, a Kinesis Data Firehose delivery stream, or an AWS Lambda function.</td></tr>\n",
    " <tr><td>Amazon Redshift</td><td>size and number of nodes of your cluster. No additional charge for backup storage up to 100% of your provisioned storage. Backup storage beyond the provisioned storage size, and backups stored after your cluster is terminated, are billed at standard Amazon S3 rates. No data transfer charge for communication between Amazon S3 and Amazon Redshift.</td><td>Columnar storage, data compression, and zone maps to reduce the amount of I/O needed to perform queries. 10 GigE mesh network to maximize throughput between nodes.</td><td>Amazon Redshift automatically detects and replaces a failed node in your data warehouse cluster. The data warehouse cluster is read-only until a replacement node is provisioned and added to the DB, which typically only takes a few minutes. </td><td>You can change the number or type of nodes in your data warehouse. You can use elastic resize to scale your cluster by changing the number of nodes. Or, you can use classic resize to scale the cluster by specifying a different node type</td><td>JDBC and ODBC. You can ingest data into your data warehouse cluster as well as to and from Amazon S3 and DynamoDB.You can load streaming data into Amazon Redshift using Amazon Kinesis Data Firehose.</td></tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "\ttable.tableizer-table {\n",
    "\t\tfont-size: 12px;\n",
    "\t\tborder: 1px solid #CCC; \n",
    "\t\tfont-family: Arial, Helvetica, sans-serif;\n",
    "\t} \n",
    "\t.tableizer-table td {\n",
    "\t\tpadding: 4px;\n",
    "\t\tmargin: 3px;\n",
    "\t\tborder: 1px solid #CCC;\n",
    "\t}\n",
    "\t.tableizer-table th {\n",
    "\t\tbackground-color: #104E8B; \n",
    "\t\tcolor: #FFF;\n",
    "\t\tfont-weight: bold;\n",
    "\t}\n",
    "</style>\n",
    "<table class=\"tableizer-table\">\n",
    "<thead><tr class=\"tableizer-firstrow\"><th></th><th>Ideal Patterns</th><th>Anti-Patterns</th></tr></thead><tbody>\n",
    " <tr><td>Amazon Athena</td><td><div class=\"fr-view\"><ol><li><strong>Interactive ad hoc querying for weblogs</strong> â€“ Athena is a good tool for interactive <strong>one-time SQL queries</strong> against data on Amazon S3. For example, you could use Athena to run a query on web and application logs to troubleshoot a performance issue. Athena integrates with Amazon QuickSight for easy visualization.</li><li><strong>Interactive Analytical Solutions with notebook-based solutions</strong> - Data scientists and Analysts are often concerned about managing the infrastructure behind big data platforms while running notebook-based solutions such as RStudio, Jupyter, and Zeppelin. Amazon Athena makes it easy to analyze data using standard SQL without the need to manage infrastructure.&nbsp;</li><li><strong>Analyze AWS service logs</strong>&nbsp; â€“ AWS CloudTrail, Amazon CloudFront, Elastic Load Balancing and Amazon VPC flow logs can be analyzed with Athena. The logs allow you to investigate network traffic patterns and identify threats and risks across your Amazon VPC estate.</li><li><strong>Query staging data before loading into Amazon Redshift</strong> â€“ You can stage your raw data in Amazon S3 before processing and loading it into Amazon Redshift, and then use Athena to query that data.</li></ol></div></td><td><div class=\"fr-view\"><ol><li><strong>Enterprise Reporting and Business Intelligence Workloads</strong> â€“ <strong>Amazon Redshift is a better tool&nbsp;</strong>for Enterprise Reporting and Business Intelligence Workloads involving iceberg queries or cached data at the nodes. Data warehouses pull data from many sources, format and organize it, store it, and support complex, high speed queries that produce business reports. The query engine in Amazon Redshift has been optimized to perform especially well on data warehouse workloads.</li><li><strong>ETL Workloads</strong> â€“ You should use <strong>Amazon EMR/AWS Glue</strong> if you are looking for an ETL tool to process extremely large datasets and analyze them with the latest big data processing frameworks such as Spark, Hadoop, Presto, or Hbase.</li><li><strong>RDBMS</strong> â€“ Athena is not a relational/transactional database. It is not meant to be a replacement for SQL engines like MySQL.</li></ol></div></td></tr>\n",
    " <tr><td>Amazon ES</td><td><div class=\"fr-view\"><ol><li><strong>Analyze activity logs</strong>, e.g., logs for customer facing applications or websites</li><li><strong>Analyze CloudWatch logs</strong></li><li><strong>Analyze product usage data</strong> coming from various services and systems</li><li><strong>Analyze social media sentiments</strong>, CRM data and find trends for your brand and products</li><li><strong>Analyze data stream updates from other AWS service</strong>s, e.g., Amazon Kinesis Data Streams and Amazon DynamoDB</li><li><strong>Provide customers a rich search</strong> and navigation experience.</li><li><strong>Usage monitoring</strong> for mobile applications</li></ol></div></td><td><div class=\"fr-view\"><ol><li><strong>Online transaction processing (OLTP)</strong> - Amazon ES is a real-time distributed search and analytics engine. There is no support for transactions or processing on data manipulation. If your requirement is for a fast transactional system, then a relational database system built on <strong>Amazon RDS</strong>, or a <strong>non-relational database</strong> offering functionality such as <strong>DynamoDB</strong>, is a better choice.</li><li><strong>Ad hoc data querying&nbsp;</strong>â€“ While Amazon ES takes care of the operational overhead of building a highly scalable Elasticsearch cluster if running Ad hoc queries or one-off queries against your data set is your use-case, <strong>Amazon Athena is a better choice</strong>.</li></ol></div></td></tr>\n",
    " <tr><td>Amazon EMR</td><td><div class=\"fr-view\"><p>Amazon EMRâ€™s flexible framework <strong>reduces large processing problems</strong> and data sets into <strong>smaller jobs</strong> and <strong>distributes them&nbsp;</strong>across many compute nodes in a Hadoop cluster. This capability lends itself to many usage patterns with big data analytics including:</p><ol><li><strong>Log processing and analytics</strong></li><li><strong>Large extract, transform, and load</strong> (ETL) data movement</li><li><strong>Risk modeling</strong> and <strong>threat analytics</strong></li><li><strong>Ad targeting</strong> and <strong>clickstream analytics</strong></li><li><strong>Genomics</strong></li><li><strong>Predictive analytics</strong></li><li><strong>Ad hoc data mining</strong> and analytics</li></ol></div></td><td><div class=\"fr-view\"><ol><li><strong>Small data sets&nbsp;</strong>â€“ Amazon EMR is <strong>built for massively parallel processing</strong>; if your data set is small enough to run quickly on a single machine, in a single thread, the added overhead to map and reduce jobs may not be worth it for small data sets that can easily be processed in memory on a single system.</li><li><strong>ACID transaction requirements</strong> â€“ While there are ways to achieve ACID (atomicity, consistency, isolation, durability) or limited ACID on Hadoop, using another database, such as Amazon RDS or a relational database running on Amazon EC2 may be a better option for workloads with stringent requirements.</li></ol></div></td></tr>\n",
    " <tr><td>Amazon Kinesis</td><td><div class=\"fr-view\"><ol><li><strong>Real-time data analytics</strong> â€“ Kinesis Data Streams enables real-time data analytics on streaming data, such as analyzing <strong>website clickstream data</strong> and <strong>customer engagement</strong> analytics.</li><li><strong>Log and data feed intake and processing</strong> â€“ With Kinesis Data Streams, you can have producers <strong>push data directly into an Amazon Kinesis data stream</strong>. For example, you can submit system and application logs to Kinesis Data Streams and access the stream for processing within seconds. This <strong>prevents the log data from being lost if the front-end or application server fails</strong>, and <strong>reduces local log storage</strong> on the source. Kinesis Data Streams provides <strong>accelerated data intake&nbsp;</strong>because you are not batching up the data on the servers before you submit it for intake.</li><li><strong>Real-time metrics and reporting</strong> â€“ You can use data ingested into Kinesis Data Streams for extracting metrics and generating KPIs to power reports and dashboards at real-time speeds. This enables data-processing application logic to work on data as it is streaming in continuously, rather than wait for data batches to arrive.</li></ol></div></td><td><div class=\"fr-view\"><ol><li><strong>Small scale consistent throughput&nbsp;</strong>â€“ Even though Kinesis Data Streams works for streaming data at 200 KB/sec or less, it is designed and optimized for<strong>&nbsp;larger data throughputs</strong>.</li><li><strong>Long-term data storage and analytics</strong> â€“Kinesis Data Streams is not suited for long-term data storage. By default, data is retained for 24 hours, and you can extend the retention period by up to 7 days. You can move any data that needs to be stored for longer than 7 days into another durable storage service such as Amazon S3, Amazon S3 Glacier, Amazon Redshift, or DynamoDB.</li></ol></div></td></tr>\n",
    " <tr><td>Amazon Redshift</td><td><div class=\"fr-view\"><p>Amazon Redshift is ideal for<strong> online analytical processing (OLAP)</strong> using your existing business intelligence tools. Organizations are using Amazon Redshift to:</p><ol><li><strong>Analyze global sales data</strong> for multiple products</li><li>Store <strong>historical stock trade data</strong></li><li><strong>Analyze ad impressions</strong> and clicks</li><li><strong>Aggregate gaming data</strong></li><li><strong>Analyze social trends</strong></li><li><strong>Measure clinical quality</strong>, <strong>operation efficiency</strong>, and <strong>financial performance</strong> in health care</li></ol></div></td><td><div class=\"fr-view\"><ol><li><strong>Small data sets&nbsp;</strong>â€“ Amazon Redshift is <strong>built for parallel processing across a cluster</strong>. If your data set is <strong>less than a hundred gigabytes</strong>, you are <strong>not going to get all the benefits</strong> that Amazon Redshift has to offer and <strong>Amazon RDS may be a better solution</strong>.</li><li><strong>On-line transaction processing (OLTP)</strong> â€“ Amazon Redshift is<strong>&nbsp;designed for data warehouse workloads&nbsp;</strong>producing <strong>extremely fast and inexpensive analytic capabilities</strong>. If you require a fast transactional system, you may want to choose a traditional relational database system built on <strong>Amazon RDS</strong> or a Non-relational database offering, such as <strong>DynamoDB</strong>.</li></ol></div></td></tr>\n",
    " <tr><td>Amazon QuickSight</td><td><div class=\"fr-view\"><p>Amazon QuickSight is an ideal Business Intelligence tool allowing end-users to create visualizations that provide insight into their data to help them make better business decisions. Use cases include:</p><ol><li>Quick <strong>interactive ad-hoc exploration&nbsp;</strong>and <strong>optimized visualization</strong> of data</li><li>Create and share <strong>dashboards and KPIâ€™s</strong> to provide insight into your data</li><li>Create<strong>&nbsp;Stories&nbsp;</strong>which are <strong>guided tours through specific views of an analysis</strong> and allow you to share insights and collaborate with others. They are used to convey key points, a thought process, or the evolution of analysis for collaboration.</li><li><strong>Analyze and&nbsp;</strong><strong>visualize data coming from logs and stored in Amazon S3</strong></li><li><strong>Analyze and visualize data from on-premises databases</strong> like SQL Server, Oracle, PostgreSQL, and MySQL</li><li><strong>Analyze and visualize data in various AWS resources</strong>, e.g., Amazon RDS databases, Amazon Redshift, Amazon Athena, and Amazon S3.</li><li><strong>Analyze and visualize data in software as a service (SaaS) applications&nbsp;</strong>like Salesforce.</li><li><strong>Analyze and visualize data in data sources</strong> that can be connected to <strong>using JDBC/ODBC</strong> connection<strong>.</strong></li></ol></div></td><td><div class=\"fr-view\"><ol><li><strong>Highly formatted canned Reports</strong> â€“ Amazon QuickSight is much more suited for ad hoc query, analysis and visualization of data. For highly formatted reports e.g. formatted financial statements consider using a different tool.</li><li><strong>ETL</strong> - While Amazon QuickSight can perform some transformations <strong>it is not a full-fledged ETL tool</strong>. AWS offers <strong>AWS Glue</strong>, which is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics.</li></ol></div></td></tr>\n",
    " <tr><td>Amazon SageMaker</td><td><div class=\"fr-view\"><ol><li><strong>Enable applications to flag suspicious transactions</strong> â€“ Build an ML model that predicts whether a new transaction is legitimate or fraudulent.</li><li><strong>Forecast product demand</strong> â€“ Input historical order information to predict future order quantities.</li><li><strong>Personalize application content</strong> â€“ Predict which items a user will be most interested in, and retrieve these predictions from your application in real-time.</li><li><strong>Predict user activity&nbsp;</strong>â€“ Analyze user behavior to customize your website and provide a better user experience.</li><li><strong>Listen to social media</strong> â€“ Ingest and analyze social media feeds that potentially impact business decisions.</li></ol></div></td><td><div class=\"fr-view\"><ol><li><strong>Very large data sets</strong> â€“ Terabyte-scale ingestion of data is not currently supported. Using Amazon EMR to run Sparkâ€™s Machine Learning Library (MLlib) is a common tool for such a use case.</li><li>Cases, where you need full control over your ML environment,&nbsp;are not ideal for Amazon SageMaker since it is a managed service.</li></ol></div></td></tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"src/AWS_Analytics_Services.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon QuickSight visual types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cost\n",
    "Amazon QuickSight has two different editions for pricing; standard edition and enterprise edition. Pricing is based on an annual subscription. Both standard and enterprise editions include SPICE (Super-fast, Parallel, and In-memory Calculation Engine) capacity, and you can get additional SPICE capacity for a monthly add on cost. Month to month billing options are available for both the editions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance\n",
    "Amazon QuickSight is built with SPICE. Built from the ground up for the cloud, SPICE uses a combination of columnar storage, in-memory technologies enabled through the latest hardware innovations, and machine code generation to run interactive queries on large datasets and get rapid responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Durability and availability\n",
    "SPICE automatically replicates data for high availability and enables Amazon QuickSight to scale to hundreds of thousands of users who can all simultaneously perform fast interactive analysis across a wide variety of AWS data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scalability and elasticity\n",
    "QuickSight is a fully managed service that internally takes care of scaling to meet the demands of end-users. You can seamlessly grow your data from a few hundred megabytes to many terabytes of data without managing any infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interfaces\n",
    "Amazon QuickSight can connect to a wide variety of data sources including flat files (CSV, TSV, CLF, ELF),  on-premises databases like SQL Server, MySQL, and PostgreSQL, and AWS data sources including Amazon RDS, Amazon Aurora, Amazon Redshift, Amazon Athena and Amazon S3, and SaaS applications like Salesforce. You can also export analyzes from a visual to a file with CSV format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%;\"><thead><tr><th style=\"width:26.3698%;background-color:rgb(13, 78, 70);\"><span style=\"color:rgb(255, 255, 255);font-weight:bold;font-size:16px;\">Type</span></th><th style=\"width:73.4989%;background-color:rgb(13, 78, 70);\"><span style=\"color:rgb(255, 255, 255);font-weight:bold;font-size:16px;\">Use</span><br></th></tr></thead><tbody><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/autograph.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">AutoGraph<br></span></a></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">AutoGraph isn't a visual type itself but instead lets you tell Amazon QuickSight to choose the visual type for you.</span></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/bar-charts.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">Bar charts</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use a horizontal or vertical bar chart to create single-measure, multi-measure, or clustered bar charts. Use any of the stacked bar chart visual types to create stacked bar charts.</span></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/combo-charts.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">Combo charts</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use the combo chart visual types to create a single visualization that shows two different types of data. &nbsp;These are also known as line and column charts. Amazon QuickSight supports clustered bar combo and stacked bar combo charts.</span><br></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/donut-chart.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">Donut charts</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use donut charts to compare values for items in a dimension. The best use for this type of chart is to show a percentage of the total amount. &nbsp;</span><br></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/gauge-chart.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">Gauge charts</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use gauge charts to compare values for items in a measure. A gauge chart is similar to a non-digital gauge, for example, a gas gauge in an automobile. It displays how much there is of the thing you are measuring.</span><br></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/geospatial-charts.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">Geospatial charts (maps)</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use geospatial charts to show differences in data values across a geographical map. The map allows you to zoom in and out.</span><br></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/heat-map.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">Heat maps</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use heat maps to show a measure for the intersection of two dimensions, with color-coding to easily differentiate where values fall in the range.</span><br></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/kpi.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">KPIs</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use a KPI to visualize a comparison between a key-value and its target value.</span><br></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/line-charts.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">Line charts</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use line charts to compare changes in measure values over a period of time.</span><br></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/pie-chart.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">Pie charts</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use pie charts to compare values for items in a dimension. The best use for this type of chart is to show a percentage of the total amount. &nbsp; &nbsp; &nbsp;</span><br></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/pivot-table.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">Pivot tables</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use pivot tables to show measure values for the intersection of two dimensions. Heat maps and pivot tables display data in a similar tabular fashion. Use a heat map if you want to identify trends and outliers because the use of color makes these easier to spot. Use a pivot table if you want to analyze data on the visual. &nbsp; &nbsp;</span><br></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/scatter-plot.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">Scatter plots</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use scatter plots to visualize two or three measures for a dimension.</span><br></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/tabular.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">Tables</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use a table visual to see a customized table view of your data. To create a table visual, choose at least one field of any data type.</span><br></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/tree-map.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">Tree maps</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use tree maps to visualize one or two measures for a dimension. Each rectangle on the tree represents one item in the dimension.</span><br></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/word-cloud.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">Word clouds</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use word clouds as an engaging way to display how often a word is used in relation to other words in a data set.</span><br></td></tr></tbody></table>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAQs on all the services which revolve around Lake Formation. i.e Glue, Athena, EMR, Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kinesis, S3, Lambda, EMR, Glue, Athena, Redshift and Quicksight\n",
    "\n",
    "Redshift, EMR, and Kinesis because they test you on every detail\n",
    "\n",
    "Redshift, Athena, Glue, Kinesis, EMR and Quicksight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Security (18%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five different encryption options available to encrypt your S3 objects, as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "\ttable.tableizer-table {\n",
    "\t\tfont-size: 12px;\n",
    "\t\tborder: 1px solid #CCC; \n",
    "\t\tfont-family: Arial, Helvetica, sans-serif;\n",
    "\t} \n",
    "\t.tableizer-table td {\n",
    "\t\tpadding: 4px;\n",
    "\t\tmargin: 3px;\n",
    "\t\tborder: 1px solid #CCC;\n",
    "\t}\n",
    "\t.tableizer-table th {\n",
    "\t\tbackground-color: #104E8B; \n",
    "\t\tcolor: #FFF;\n",
    "\t\tfont-weight: bold;\n",
    "\t}\n",
    "</style>\n",
    "<table class=\"tableizer-table\">\n",
    "<thead><tr class=\"tableizer-firstrow\"><th>Encryption Option</th><th>Name</th><th>Protocol</th></tr></thead><tbody>\n",
    " <tr><td>SSE-S3</td><td>Server-side encryption with S3 managed keys</td><td>http/s</td></tr>\n",
    " <tr><td>SSE-KMS</td><td>Server-side encryption with KMS managed keys</td><td>http/s</td></tr>\n",
    " <tr><td>SSE-C</td><td>Server-side encryption with customer-managed keys</td><td>only https</td></tr>\n",
    " <tr><td>CSE-KMS</td><td>Client-side encryption with KMS managed keys</td><td>http/s</td></tr>\n",
    " <tr><td>CSE-C</td><td>Client-side encryption with customer-managed keys</td><td>http/s</td></tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between server-side and client-side encryption is fairly simple. With serverside encryption, the encryption algorithm and process is run from the server-sideâ€”in this\n",
    "instance, within S3. Client-side encryption means that the encryption process is executed on\n",
    "the client first before the data is sent to S3 for storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS Kinesis Data Firehose\n",
    "https://docs.aws.amazon.com/firehose/latest/dev/encryption.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS Athena Data protection\n",
    "https://docs.aws.amazon.com/athena/latest/ug/encryption.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS Glue https://docs.aws.amazon.com/glue/latest/dg/data-protection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS Redshift https://docs.aws.amazon.com/redshift/latest/mgmt/security-data-protection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AWS CloudHMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"src/KMSvsHMS.png\" width=\"779\" height=\"196\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS EMR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://aws.amazon.com/blogs/big-data/best-practices-for-securing-amazon-emr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
