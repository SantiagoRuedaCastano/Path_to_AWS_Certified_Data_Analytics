{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path to AWS Certified Data Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"src/Domains.png\" width=\"779\" height=\"196\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect (18%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "\ttable.tableizer-table {\n",
    "\t\tfont-size: 12px;\n",
    "\t\tborder: 1px solid #CCC; \n",
    "\t\tfont-family: Arial, Helvetica, sans-serif;\n",
    "\t} \n",
    "\t.tableizer-table td {\n",
    "\t\tpadding: 4px;\n",
    "\t\tmargin: 3px;\n",
    "\t\tborder: 1px solid #CCC;\n",
    "\t}\n",
    "\t.tableizer-table th {\n",
    "\t\tbackground-color: #104E8B; \n",
    "\t\tcolor: #FFF;\n",
    "\t\tfont-weight: bold;\n",
    "\t}\n",
    "</style>\n",
    "<table class=\"tableizer-table\" > \n",
    "           <tbody> \n",
    "            <tr class=\"tableizer-firstrow\"> \n",
    "             <th width=\"15%\">&nbsp;</th> \n",
    "             <th style=\"text-align: center;\">Batch processing</th> \n",
    "             <th style=\"text-align: center;\">Stream processing<br> </th> \n",
    "            </tr> \n",
    "            <tr> \n",
    "             <td width=\"15%\">Data scope</td> \n",
    "             <td style=\"text-align: left;\">Queries or processing over all or most of the data in the dataset.</td> \n",
    "             <td style=\"text-align: left;\">Queries or processing over data within a rolling time window, or on just the most recent data record.</td> \n",
    "            </tr> \n",
    "            <tr> \n",
    "             <td width=\"15%\">Data size</td> \n",
    "             <td style=\"text-align: left;\">Large batches of data.</td> \n",
    "             <td style=\"text-align: left;\">Individual records or micro batches consisting of a few records.</td> \n",
    "            </tr> \n",
    "            <tr> \n",
    "             <td width=\"15%\">Performance</td> \n",
    "             <td style=\"text-align: left;\">Latencies in minutes to hours.</td> \n",
    "             <td style=\"text-align: left;\">Requires latency in the order of seconds or milliseconds.</td> \n",
    "            </tr> \n",
    "            <tr> \n",
    "             <td width=\"15%\">Analyses</td> \n",
    "             <td style=\"text-align: left;\">Complex analytics.</td> \n",
    "             <td style=\"text-align: left;\">Simple response functions, aggregates, and rolling metrics.</td> \n",
    "            </tr> \n",
    "           </tbody> \n",
    "          </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>At most once</h5>\n",
    "At most once means that a message will never be delivered more than one time. However, a message could be lost. Note that, this\n",
    "pattern is only accepted when the data itself is low value or loses value if it is not immediately processed.\n",
    "<h5>At least once</h5>\n",
    "delivery replays recent events starting from an acknowledged (known processed) event.\n",
    "This approach presents some data more than once to the processing pipeline. The typical implementation\n",
    "returns at-least-once delivery checkpoints to a safe point (so you know that they have been processed).\n",
    "<h5>Exactly Once</h5> \n",
    "This type of processing is the ideal because each event is processed exactly once. It avoids the difficult side\n",
    "effects and considerations raised by the other two deliveries. You can achieve\n",
    "exactly-once semantics using idempotency in combination with at-least-once delivery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review operational characteristics of AWS ingestion services\n",
    "\n",
    "<style type=\"text/css\">\n",
    "\ttable.tableizer-table {\n",
    "\t\tfont-size: 12px;\n",
    "\t\tborder: 1px solid #CCC; \n",
    "\t\tfont-family: Arial, Helvetica, sans-serif;\n",
    "\t} \n",
    "\t.tableizer-table td {\n",
    "\t\tpadding: 4px;\n",
    "\t\tmargin: 3px;\n",
    "\t\tborder: 1px solid #CCC;\n",
    "\t}\n",
    "\t.tableizer-table th {\n",
    "\t\tbackground-color: #104E8B; \n",
    "\t\tcolor: #FFF;\n",
    "\t\tfont-weight: bold;\n",
    "\t}\n",
    "</style>\n",
    "<table class=\"tableizer-table\">\n",
    "<thead><tr class=\"tableizer-firstrow\"><th></th><th>Amazon Dynamo DB Streams</th><th>Amazon Kinesis Data Stream</th><th>Amazon Kinesis Data Firehose</th><th>Amazon MSK</th><th>Apache Kafka(EC2)</th><th>Amazon SQS Standard</th><th>Amazon SQS FIFO</th></tr></thead><tbody>\n",
    " <tr><td>AWS Managed</td><td>Yes</td><td>Yes</td><td>Yes</td><td>Yes</td><td>No</td><td>Yes</td><td>Yes</td></tr>\n",
    " <tr><td>Use cases</td><td><ul><li>Replication</li><li>Mobile app</li><li>Global multi-player game</li></ul></td><td> <ul><li>Key-Value</li><li>Document store</li></ul></td><td>Loader streaming data into data lakes, data stores and analytics tools</td><td>real-time streaming data pipelines and applications</td><td>real-time streaming data pipelines and applications</td><td>Queue</td><td>Queue</td></tr>\n",
    " <tr><td>Guaranteed ordering</td><td>Yes</td><td>Yes</td><td>No</td><td>Yes</td><td>Yes</td><td>No</td><td>Yes</td></tr>\n",
    " <tr><td>Delivery(deduping)</td><td>Exactly once</td><td>At least once</td><td>At least once</td><td>At least once</td><td>At least once</td><td>At least once</td><td>Exactly once</td></tr>\n",
    " <tr><td>Data retention period</td><td>24 hours</td><td>7 days</td><td>N/A</td><td>Configurable</td><td>Configurable</td><td>14 days</td><td>14 days</td></tr>\n",
    " <tr><td>Availability</td><td>3 AZ</td><td>3 AZ</td><td>3 AZ</td><td>Configurable Multi AZ</td><td>Configurable Multi AZ</td><td>3 AZ</td><td>3 AZ</td></tr>\n",
    " <tr><td>Scale/throughput</td><td>No limit / Automatic</td><td>No limit / shards</td><td>No limit / Automatic</td><td>up to 30 / brokers</td><td>No limit / nodes</td><td>No limit / Automatic</td><td>300 TPS / queue</td></tr>\n",
    " <tr><td>Parallel Consumption</td><td>Yes</td><td>Yes</td><td>No</td><td>Yes</td><td>Yes</td><td>No</td><td>No</td></tr>\n",
    " <tr><td>Row/Object Size</td><td>400 KB</td><td>1 MB</td><td>Destination row / object size</td><td>Varies</td><td>Varies</td><td>256 KB</td><td>256 KB</td></tr>\n",
    " <tr><td>Stream MapReduce</td><td>Yes</td><td>Yes</td><td>N/A</td><td>Yes</td><td>Yes</td><td>N/A</td><td>N/A</td></tr>\n",
    "<tr><td>Cost</td><td>Higher</td><td>Low</td><td>Low</td><td>Low</td><td>Low(+Admin)</td><td>Low-Medium</td><td>Low-Medium</td></tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon MSK: up to 30 brokers, but, you can request a limit increase in the AWS Support Center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <ol>\n",
    "  <li>How quickly do you need the analytics results?  In real time, in seconds, or is an hour a more appropriate time frame?</li>\n",
    "  <li>Where is the data coming from?</li>\n",
    "</ol> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing ingestion services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "\ttable.tableizer-table {\n",
    "\t\tfont-size: 12px;\n",
    "\t\tborder: 1px solid #CCC; \n",
    "\t\tfont-family: Arial, Helvetica, sans-serif;\n",
    "\t} \n",
    "\t.tableizer-table td {\n",
    "\t\tpadding: 4px;\n",
    "\t\tmargin: 3px;\n",
    "\t\tborder: 1px solid #CCC;\n",
    "\t}\n",
    "\t.tableizer-table th {\n",
    "\t\tbackground-color: #104E8B; \n",
    "\t\tcolor: #FFF;\n",
    "\t\tfont-weight: bold;\n",
    "\t}\n",
    "</style>\n",
    "<table class=\"tableizer-table\">\n",
    "<thead><tr class=\"tableizer-firstrow\"><th></th><th>Kinesis Data Streams</th><th>Kinesis Data Firehose</th><th>AWS DMS</th><th>AWS Glue</th></tr></thead><tbody>\n",
    " <tr><td>scale and throughput</td><td>Each shard can support up to 1,000 PUT records per second. However, you can increase the number of shards limitlessly. One shard provides a capacity of 1 MB/sec data input and 2 MB/sec data output</td><td>Kinesis Data Firehose will automatically scale to match the throughput of your data, without any manual intervention or developer overhead.</td><td>AWS DMS uses Amazon EC2 instances as the replication instance. You can scale up or down your replication instance.</td><td>AWS Glue uses a scale-out Apache Spark environment to load your data into its destination. To scale out, you specify the number of DPUs (data processing units) that you want to allocate to your ETL jobs.</td></tr>\n",
    " <tr><td>fault tolerance</td><td>3 AZ</td><td>3 AZ</td><td>You have the option of enabling Multi-AZ which provides a replication stream that is fault-tolerant through redundant replication servers.</td><td>AWS Glue connects to the data source of your preference, whether it is in an S3 file, RDS table, or so on. AWS Glue also provides default retry behavior that will retry all failures three times before sending out an error notification. You can set up Amazon SNS notifications via Amazon CloudWatch actions.</td></tr>\n",
    " <tr><td>cost</td><td>You pay per shard hour and per PUT payload unit. Optionally, there are fees associated with extended data retention and enhanced fan-out, if you choose to use those features.</td><td>You pay for the volume of data you ingest using the service and for any data format conversions.</td><td>You pay for compute resources (depending on instance type) used during the migration process and any additional log storage. There are also potential data transfer fees.</td><td>With AWS Glue, you pay an hourly rate, billed by the second, for crawlers (discovering data) and ETL jobs (processing and loading data).</td></tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform and filter data during collection process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Amazon Kinesis Data Firehose</h5>\n",
    "<p>When you enable Kinesis Data Firehose data transformation, Kinesis Data Firehose buffers incoming data and invokes the specified AWS Lambda function with each buffered batch asynchronously. The transformed data is sent from Lambda to Kinesis Data Firehose for buffering and then delivered to the destination. You can also choose to enable source record backup, which backs up all untransformed records to your S3 bucket concurrently while delivering transformed records to the destination.</p>\n",
    "<h5>AWS Lambda</h5>\n",
    "<p>You can also use an AWS Lambda function separately to provide format conversion, transformation, and filtering for the data in your stream. &nbsp;Using a Lambda function for preprocessing records is useful in the following scenarios:Transforming records from other formats, Expanding data, Data enrichment, Data Filtering and String transformations.</p>\n",
    "<h5>AWS Database Migration Service</h5> \n",
    "<p>AWS DMS offers enhanced data transformation capabilities for table and schema migrations. You can change schema, table, and column names during migration using explicit selections, specify the name of the tablespace in which you want the table or table index to be created for an Oracle target, and update the primary key and unique key for a table on any <a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">supported target</a>. It also supports Apache Parquet format (.parquet) when migrating data to Amazon S3 for more compact storage and faster queries. This format is supported for both full load and change data capture (CDC).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage and Data Management (22%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "\ttable.tableizer-table {\n",
    "\t\tfont-size: 12px;\n",
    "\t\tborder: 1px solid #CCC; \n",
    "\t\tfont-family: Arial, Helvetica, sans-serif;\n",
    "\t} \n",
    "\t.tableizer-table td {\n",
    "\t\tpadding: 4px;\n",
    "\t\tmargin: 3px;\n",
    "\t\tborder: 1px solid #CCC;\n",
    "\t}\n",
    "\t.tableizer-table th {\n",
    "\t\tbackground-color: #104E8B; \n",
    "\t\tcolor: #FFF;\n",
    "\t\tfont-weight: bold;\n",
    "\t}\n",
    "</style>\n",
    "<table class=\"tableizer-table\">\n",
    "<thead><tr class=\"tableizer-firstrow\"><th></th><th>Amazon ElastiCache</th><th>Amazon DynamoDB + Amazon DynamoDB Accelerator (DAX)</th><th>Amazon Aurora</th><th>Amazon RDS </th><th>Amazon Neptune</th><th>Amazon S3 + Amazon S3 Glacier </th></tr></thead><tbody>\n",
    " <tr><td>Use cases</td><td>In-memory caching</td><td>K/V lookups, document store</td><td>OLTP, transactional</td><td>OLTP, transactional</td><td>Graph</td><td>File Store</td></tr>\n",
    " <tr><td>Throughput</td><td>Ultra-high request rate, Ultra-low latency</td><td>Ultra-high request rate, Ultra-low to low latency </td><td>Very high request rate, low latency</td><td>High request rate, low latency</td><td>Medium request rate, low latency</td><td>High throughput</td></tr>\n",
    " <tr><td>Shape</td><td>K/V</td><td>K/V and document</td><td>Relational</td><td>Relational</td><td>Node/edges</td><td>Files</td></tr>\n",
    " <tr><td>Size</td><td>GB</td><td>TB, PB (no limits)</td><td>GB, mid TB</td><td>GB, low TB</td><td>GB, mid TB</td><td>GB, TB, PB, EB (no limits)</td></tr>\n",
    " <tr><td>Cost/GB</td><td><span>&#36;</span><span>&#36;</span></td> <td><span>&#162;</span><span>&#162;</span> - <span>&#36;</span><span>&#36;</span></td> <td><span>&#162;</span><span>&#162;</span></td><td><span>&#162;</span><span>&#162;</span></td><td><span>&#162;</span><span>&#162;</span></td><td><span>&#162;</span></td></tr>\n",
    " <tr><td>Availability</td><td>2 AZ</td><td>3 AZ</td><td>3 AZ</td><td>2 AZ</td><td>3 AZ</td><td>3 AZ</td></tr>\n",
    " <tr><td>Amazon Virtual Private Cloud (VPC)support</td><td>Inside VPC</td><td>DynamoDB has a VPC endpoint and DAX is inside VPC</td><td>Inside VPC</td><td>Inside VPC</td><td>Inside VPC</td><td>VPC endpoint</td></tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing (24 %)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
