{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path to AWS Certified Data Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"src/Domains.png\" width=\"779\" height=\"196\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect (18%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "\ttable.tableizer-table {\n",
    "\t\tfont-size: 12px;\n",
    "\t\tborder: 1px solid #CCC; \n",
    "\t\tfont-family: Arial, Helvetica, sans-serif;\n",
    "\t} \n",
    "\t.tableizer-table td {\n",
    "\t\tpadding: 4px;\n",
    "\t\tmargin: 3px;\n",
    "\t\tborder: 1px solid #CCC;\n",
    "\t}\n",
    "\t.tableizer-table th {\n",
    "\t\tbackground-color: #104E8B; \n",
    "\t\tcolor: #FFF;\n",
    "\t\tfont-weight: bold;\n",
    "\t}\n",
    "</style>\n",
    "<table class=\"tableizer-table\" > \n",
    "           <tbody> \n",
    "            <tr class=\"tableizer-firstrow\"> \n",
    "             <th width=\"15%\">&nbsp;</th> \n",
    "             <th style=\"text-align: center;\">Batch processing</th> \n",
    "             <th style=\"text-align: center;\">Stream processing<br> </th> \n",
    "            </tr> \n",
    "            <tr> \n",
    "             <td width=\"15%\">Data scope</td> \n",
    "             <td style=\"text-align: left;\">Queries or processing over all or most of the data in the dataset.</td> \n",
    "             <td style=\"text-align: left;\">Queries or processing over data within a rolling time window, or on just the most recent data record.</td> \n",
    "            </tr> \n",
    "            <tr> \n",
    "             <td width=\"15%\">Data size</td> \n",
    "             <td style=\"text-align: left;\">Large batches of data.</td> \n",
    "             <td style=\"text-align: left;\">Individual records or micro batches consisting of a few records.</td> \n",
    "            </tr> \n",
    "            <tr> \n",
    "             <td width=\"15%\">Performance</td> \n",
    "             <td style=\"text-align: left;\">Latencies in minutes to hours.</td> \n",
    "             <td style=\"text-align: left;\">Requires latency in the order of seconds or milliseconds.</td> \n",
    "            </tr> \n",
    "            <tr> \n",
    "             <td width=\"15%\">Analyses</td> \n",
    "             <td style=\"text-align: left;\">Complex analytics.</td> \n",
    "             <td style=\"text-align: left;\">Simple response functions, aggregates, and rolling metrics.</td> \n",
    "            </tr> \n",
    "           </tbody> \n",
    "          </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>At most once</h5>\n",
    "At most once means that a message will never be delivered more than one time. However, a message could be lost. Note that, this\n",
    "pattern is only accepted when the data itself is low value or loses value if it is not immediately processed.\n",
    "<h5>At least once</h5>\n",
    "delivery replays recent events starting from an acknowledged (known processed) event.\n",
    "This approach presents some data more than once to the processing pipeline. The typical implementation\n",
    "returns at-least-once delivery checkpoints to a safe point (so you know that they have been processed).\n",
    "<h5>Exactly Once</h5> \n",
    "This type of processing is the ideal because each event is processed exactly once. It avoids the difficult side\n",
    "effects and considerations raised by the other two deliveries. You can achieve\n",
    "exactly-once semantics using idempotency in combination with at-least-once delivery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review operational characteristics of AWS ingestion services\n",
    "\n",
    "<style type=\"text/css\">\n",
    "\ttable.tableizer-table {\n",
    "\t\tfont-size: 12px;\n",
    "\t\tborder: 1px solid #CCC; \n",
    "\t\tfont-family: Arial, Helvetica, sans-serif;\n",
    "\t} \n",
    "\t.tableizer-table td {\n",
    "\t\tpadding: 4px;\n",
    "\t\tmargin: 3px;\n",
    "\t\tborder: 1px solid #CCC;\n",
    "\t}\n",
    "\t.tableizer-table th {\n",
    "\t\tbackground-color: #104E8B; \n",
    "\t\tcolor: #FFF;\n",
    "\t\tfont-weight: bold;\n",
    "\t}\n",
    "</style>\n",
    "<table class=\"tableizer-table\">\n",
    "<thead><tr class=\"tableizer-firstrow\"><th></th><th>Amazon DynamoDB Streams</th><th>Amazon Kinesis Data Stream(KDS)</th><th>Amazon Kinesis Data Firehose</th><th>Amazon MSK</th><th>Apache Kafka(on EC2)</th><th>Amazon SQS Standard</th><th>Amazon SQS FIFO</th></tr></thead><tbody>\n",
    " <tr><td>AWS Managed</td><td>Yes</td><td>Yes</td><td>Yes</td><td>Yes</td><td>No</td><td>Yes</td><td>Yes</td></tr>\n",
    " <tr><td>Use cases</td><td><ul><li>Replication</li><li>Mobile app</li><li>Global multi-player game</li></ul></td><td> real-time data streaming</td><td>Loader streaming data into data lakes, data stores and analytics tools</td><td>real-time streaming data pipelines and applications</td><td>real-time streaming data pipelines and applications</td><td>Queue</td><td>Queue</td></tr>\n",
    " <tr><td>Guaranteed ordering</td><td>Yes</td><td>Yes</td><td>No</td><td>Yes</td><td>Yes</td><td>No</td><td>Yes</td></tr>\n",
    " <tr><td>Delivery(deduping)</td><td>Exactly once</td><td>At least once</td><td>At least once</td><td>At least once</td><td>At least once</td><td>At least once</td><td>Exactly once</td></tr>\n",
    " <tr><td>Data retention period</td><td>24 hours</td><td>7 days</td><td>N/A</td><td>Configurable</td><td>Configurable</td><td>14 days</td><td>14 days</td></tr>\n",
    " <tr><td>Availability</td><td>3 AZ</td><td>3 AZ</td><td>3 AZ</td><td>Configurable Multi AZ</td><td>Configurable Multi AZ</td><td>3 AZ</td><td>3 AZ</td></tr>\n",
    " <tr><td>Scale/throughput</td><td>No limit / Automatic</td><td>No limit / shards</td><td>No limit / Automatic</td><td>up to 30 / brokers</td><td>No limit / nodes</td><td>No limit / Automatic</td><td>300 TPS / queue</td></tr>\n",
    " <tr><td>Parallel Consumption</td><td>Yes</td><td>Yes</td><td>No</td><td>Yes</td><td>Yes</td><td>No</td><td>No</td></tr>\n",
    " <tr><td>Row/Object Size</td><td>400 KB</td><td>1 MB</td><td>Destination row / object size</td><td>Varies</td><td>Varies</td><td>256 KB</td><td>256 KB</td></tr>\n",
    " <tr><td>Stream MapReduce</td><td>Yes</td><td>Yes</td><td>N/A</td><td>Yes</td><td>Yes</td><td>N/A</td><td>N/A</td></tr>\n",
    "<tr><td>Cost</td><td>Higher</td><td>Low</td><td>Low</td><td>Low</td><td>Low(+Admin)</td><td>Low-Medium</td><td>Low-Medium</td></tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon MSK: up to 30 brokers, but, you can request a limit increase in the AWS Support Center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <ol>\n",
    "  <li>How quickly do you need the analytics results?  In real time, in seconds, or is an hour a more appropriate time frame?</li>\n",
    "  <li>Where is the data coming from?</li>\n",
    "</ol> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing ingestion services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "\ttable.tableizer-table {\n",
    "\t\tfont-size: 12px;\n",
    "\t\tborder: 1px solid #CCC; \n",
    "\t\tfont-family: Arial, Helvetica, sans-serif;\n",
    "\t} \n",
    "\t.tableizer-table td {\n",
    "\t\tpadding: 4px;\n",
    "\t\tmargin: 3px;\n",
    "\t\tborder: 1px solid #CCC;\n",
    "\t}\n",
    "\t.tableizer-table th {\n",
    "\t\tbackground-color: #104E8B; \n",
    "\t\tcolor: #FFF;\n",
    "\t\tfont-weight: bold;\n",
    "\t}\n",
    "</style>\n",
    "<table class=\"tableizer-table\">\n",
    "<thead><tr class=\"tableizer-firstrow\"><th></th><th>Kinesis Data Streams</th><th>Kinesis Data Firehose</th><th>AWS DMS</th><th>AWS Glue</th></tr></thead><tbody>\n",
    " <tr><td>scale and throughput</td><td>Each shard can support up to 1,000 PUT records per second. However, you can increase the number of shards limitlessly. One shard provides a capacity of 1 MB/sec data input and 2 MB/sec data output</td><td>Kinesis Data Firehose will automatically scale to match the throughput of your data, without any manual intervention or developer overhead.</td><td>AWS DMS uses Amazon EC2 instances as the replication instance. You can scale up or down your replication instance.</td><td>AWS Glue uses a scale-out Apache Spark environment to load your data into its destination. To scale out, you specify the number of DPUs (data processing units) that you want to allocate to your ETL jobs.</td></tr>\n",
    " <tr><td>fault tolerance</td><td>3 AZ</td><td>3 AZ</td><td>You have the option of enabling Multi-AZ which provides a replication stream that is fault-tolerant through redundant replication servers.</td><td>AWS Glue connects to the data source of your preference, whether it is in an S3 file, RDS table, or so on. AWS Glue also provides default retry behavior that will retry all failures three times before sending out an error notification. You can set up Amazon SNS notifications via Amazon CloudWatch actions.</td></tr>\n",
    " <tr><td>cost</td><td>You pay per shard hour and per PUT payload unit. Optionally, there are fees associated with extended data retention and enhanced fan-out, if you choose to use those features.</td><td>You pay for the volume of data you ingest using the service and for any data format conversions.</td><td>You pay for compute resources (depending on instance type) used during the migration process and any additional log storage. There are also potential data transfer fees.</td><td>With AWS Glue, you pay an hourly rate, billed by the second, for crawlers (discovering data) and ETL jobs (processing and loading data).</td></tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform and filter data during collection process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Amazon Kinesis Data Firehose</h5>\n",
    "<p>When you enable Kinesis Data Firehose data transformation, Kinesis Data Firehose buffers incoming data and invokes the specified AWS Lambda function with each buffered batch asynchronously. The transformed data is sent from Lambda to Kinesis Data Firehose for buffering and then delivered to the destination. You can also choose to enable source record backup, which backs up all untransformed records to your S3 bucket concurrently while delivering transformed records to the destination.</p>\n",
    "<h5>AWS Lambda</h5>\n",
    "<p>You can also use an AWS Lambda function separately to provide format conversion, transformation, and filtering for the data in your stream. &nbsp;Using a Lambda function for preprocessing records is useful in the following scenarios:Transforming records from other formats, Expanding data, Data enrichment, Data Filtering and String transformations.</p>\n",
    "<h5>AWS Database Migration Service</h5> \n",
    "<p>AWS DMS offers enhanced data transformation capabilities for table and schema migrations. You can change schema, table, and column names during migration using explicit selections, specify the name of the tablespace in which you want the table or table index to be created for an Oracle target, and update the primary key and unique key for a table on any <a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">supported target</a>. It also supports CSV format by default and Apache Parquet format (.parquet) when migrating data to Amazon S3 for more compact storage and faster queries. This format(parquet) is supported for both full load and change data capture (CDC).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage and Data Management (22%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon RDS:<div class=\"fr-view\"><p>General characteristics:&nbsp;</p><ul><li><strong>Fast</strong><strong>:</strong>&nbsp; Choose between two SSD-backed storage options for high performance OLTP storage.</li><li><strong>Scales vertically</strong><strong>:</strong> Amazon RDS is bounded by instance and storage size limitations.</li><li><strong>Reliable and durable</strong><strong>:</strong>&nbsp;Offers Multi-AZ and automated backups, snapshots, and failover.&nbsp;</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon DynamoDB\n",
    "<div class=\"fr-view\"><p>General characteristics:&nbsp;</p><ul><li><strong>Fast</strong><strong>:</strong> Consistent single-digit millisecond latency at any scale.&nbsp;</li><li><strong>Scales horizontally</strong><strong>:</strong> Useful for storing unbounded data, providing low cost and performance regardless of size.&nbsp;</li><li><strong>Reliable and durable</strong><strong>: </strong>Data is replicated across three fault-tolerant Availability Zones with fine grained access control. &nbsp;Offers global tables for multi-region replication.&nbsp;</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon ElastiCache\n",
    "<div class=\"fr-view\"><p>General characteristics:&nbsp;</p><ul><li><strong>Extreme performance</strong><strong>: </strong>In-memory data store and cache using optimized stack to deliver sub-millisecond response times.&nbsp;</li><li><strong>Reliable and durable</strong><strong>:</strong> ElastiCache for Redis offers multi-AZ storage with automatic failover.&nbsp;</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Neptune\n",
    "<div class=\"fr-view\"><p>General characteristics:&nbsp;</p><ul><li><strong>Fast and scalable:</strong> Store billions of relationships and query with milliseconds latency.</li><li><strong>Scales vertically:&nbsp;</strong>Neptune is bounded by instance and storage size limitations.&nbsp;</li><li><strong>Reliable and durable:</strong> Six replicas of your data across three Availability Zones with full backup and restore.&nbsp;</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Redshift\n",
    "<div class=\"fr-view\"><p>General characteristics:&nbsp;</p><ul><li><strong>Fast:</strong> Columnar storage technology to improve I/O efficiency and parallelize queries. Data loads linearly. Redshift also provides fastest query results using higher cost storage.</li><li><strong>Reliable and durable:</strong> Replicates your data within your data warehouse cluster and continuously backs up your data to Amazon S3, which is designed for eleven nines of durability.</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon S3\n",
    "<div class=\"fr-view\"><p>General characteristics:&nbsp;</p><ul><li><strong>Fast:</strong> Amazon S3 storage provides a fast reliable platform for querying structured and semistructured data. Redshift spectrum and Amazon Athena are able to query data without moving it from Amazon S3 at high speed with low latency.</li><li><strong>Reliable and durable:</strong> Data is always replicated across three Availability Zones in the same region. Amazon S3 also offers same-region and &nbsp;cross-region replication for greater durability.&nbsp;</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "\ttable.tableizer-table {\n",
    "\t\tfont-size: 12px;\n",
    "\t\tborder: 1px solid #CCC; \n",
    "\t\tfont-family: Arial, Helvetica, sans-serif;\n",
    "\t} \n",
    "\t.tableizer-table td {\n",
    "\t\tpadding: 4px;\n",
    "\t\tmargin: 3px;\n",
    "\t\tborder: 1px solid #CCC;\n",
    "\t}\n",
    "\t.tableizer-table th {\n",
    "\t\tbackground-color: #104E8B; \n",
    "\t\tcolor: #FFF;\n",
    "\t\tfont-weight: bold;\n",
    "\t}\n",
    "</style>\n",
    "<table class=\"tableizer-table\">\n",
    "<thead><tr class=\"tableizer-firstrow\"><th></th><th>Amazon ElastiCache</th><th>Amazon DynamoDB + Amazon DynamoDB Accelerator (DAX)</th><th>Amazon Aurora</th><th>Amazon RDS </th><th>Amazon Neptune</th><th>Amazon S3 + Amazon S3 Glacier </th><th>Amazon Redshift</th></tr></thead><tbody>\n",
    " <tr><td>Use cases</td><td>In-memory caching</td><td>K/V lookups, document store</td><td>OLTP, transactional</td><td>OLTP, transactional</td><td>Graph</td><td>File Store</td><td>cloud data warehouse</td></tr>\n",
    " <tr><td>Throughput</td><td>Ultra-high request rate, Ultra-low latency</td><td>Ultra-high request rate, Ultra-low to low latency </td><td>Very high request rate, low latency</td><td>High request rate, low latency</td><td>Medium request rate, low latency</td><td>High throughput</td><td>High request rate, low latency</td></tr>\n",
    " <tr><td>Shape</td><td>K/V</td><td>K/V and document</td><td>Relational</td><td>Relational</td><td>Node/edges</td><td>Files</td><td>Nodes/Columnar Storage</td></tr>\n",
    " <tr><td>Size</td><td>GB</td><td>TB, PB (no limits)</td><td>GB, mid TB</td><td>GB, low TB</td><td>GB, mid TB</td><td>GB, TB, PB, EB (no limits)</td><td>TB, PB (no limits)</td></tr>\n",
    " <tr><td>Cost/GB</td><td><span>&#36;</span><span>&#36;</span></td> <td><span>&#162;</span><span>&#162;</span> - <span>&#36;</span><span>&#36;</span></td> <td><span>&#162;</span><span>&#162;</span></td><td><span>&#162;</span><span>&#162;</span></td><td><span>&#162;</span><span>&#162;</span></td><td><span>&#162;</span></td><td><span>&#162;</span><span>&#162;</span> - <span>&#36;</span><span>&#36;</span></td></tr>\n",
    " <tr><td>Availability</td><td>2 AZ</td><td>3 AZ</td><td>3 AZ</td><td>2 AZ</td><td>3 AZ</td><td>3 AZ</td><td>backup is replicated to S3 (3 AZ)</td></tr>\n",
    " <tr><td>Amazon Virtual Private Cloud (VPC)support</td><td>Inside VPC</td><td>DynamoDB has a VPC endpoint and DAX is inside VPC</td><td>Inside VPC</td><td>Inside VPC</td><td>Inside VPC</td><td>VPC endpoint</td><td>inside VPC</td></tr>\n",
    "<tr><td>Data Storage Type</td><td>Operational</td><td>Operational</td><td>Operational</td><td>Operational</td><td>Operational</td><td>Analytic</td><td>Analytic</td></tr>\n",
    "<tr><td>How long do you need to store the data</td><td>Storing transient data</td><td>Storing long-term data(DynamoDB) / Storing transient data(DAX)</td><td>Storing long-term data</td><td>Storing long-term data</td><td>Storing long-term data</td><td>Storing long-term data(S3) / Storing archived data(Glacier) </td><td>Storing long-term data</td></tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What type of data do you need to store?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%;\"><thead><tr><th style=\"width:26.6549%;background-color:rgb(13, 78, 70);\"><span style=\"color:rgb(255, 255, 255);\"><strong>Attribute</strong></span></th><th style=\"width:39.968%;background-color:rgb(13, 78, 70);\"><span style=\"color:rgb(255, 255, 255);font-weight:bold;\">Data Warehouse</span><br></th><th style=\"width:33.3333%;background-color:rgb(13, 78, 70);\"><span style=\"color:rgb(255, 255, 255);font-weight:bold;\">Data Lake</span></th></tr></thead><tbody><tr><td style=\"text-align:center;width:26.6549%;background-color:rgb(239, 239, 239);\"><span style=\"color:rgb(49, 53, 55);\"><strong>Data</strong></span></td><td style=\"text-align:center;width:39.968%;\"><strong>Relational from transactional systems, operational databases, and line of business applications</strong></td><td style=\"text-align:center;width:33.3333%;\"><strong>Nonrelational and relational from IoT devices, web sites, mobile apps, social media, and corporate applications</strong></td></tr><tr><td style=\"text-align:center;width:26.6549%;background-color:rgb(239, 239, 239);\"><strong><span style=\"color:rgb(49, 53, 55);\">Schema</span></strong><span style=\"color:rgb(255, 255, 255);\"><br></span></td><td style=\"text-align:center;width:39.968%;\"><strong>Designed prior to the implementation of the data warehouse (schema-on-write)</strong><br></td><td style=\"text-align:center;width:33.3333%;\"><strong>Written at the time of analysis (schema-on-read)</strong><br></td></tr><tr><td style=\"text-align:center;width:26.6549%;background-color:rgb(239, 239, 239);\"><span style=\"color:rgb(49, 53, 55);\"><strong>Price/performance</strong></span><span style=\"color:rgb(255, 255, 255);\"><strong><br></strong></span></td><td style=\"text-align:center;width:39.968%;\"><strong>Fastest query results using higher cost storage</strong></td><td style=\"text-align:center;width:33.3333%;\"><strong>Query results getting faster using low-cost storage</strong></td></tr><tr><td style=\"text-align:center;width:26.6549%;background-color:rgb(239, 239, 239);\"><strong><span style=\"color:rgb(49, 53, 55);\">Data quality</span></strong><span style=\"color:rgb(255, 255, 255);\"><br></span></td><td style=\"text-align:center;width:39.968%;\"><strong>Highly curated data,</strong><br><strong>serves as the central version of the truth</strong><br></td><td style=\"text-align:center;width:33.3333%;\"><strong>Raw data,</strong><br><strong>may or may not be curated</strong><br></td></tr><tr><td style=\"text-align:center;width:26.6549%;background-color:rgb(239, 239, 239);\"><strong><span style=\"color:rgb(49, 53, 55);\">Analytics</span></strong><span style=\"color:rgb(255, 255, 255);\"><br></span></td><td style=\"text-align:center;width:39.968%;\"><strong>Batch reporting, ad-hoc analysis, business intelligence, and data visualization</strong><br></td><td style=\"text-align:center;width:33.3333%;\"><strong>Machine learning, predictive analytics, data discovery, and profiling</strong><br></td></tr></tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizing Amazon Redshift "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Redshift is a massively parallel processing (MPP) system. Each Amazon Redshift cluster has one leader node, two or more compute nodes, and each compute node has a number of slices. Each node slice is an independent partition of data. The slices can perform operations in parallel to complete operations significantly faster than systems that do not use slices. Because of the massive amount of data involved in data warehousing, you must design your database to take full advantage of every performance optimization. The four types of Amazon Redshift optimizations that this section will cover are data distribution styles, sort key best practices, compression encoding implementations, and data size optimizations.\n",
    "##### Amazon Redshift data redistribution\n",
    "\n",
    "When you load data into a table, Amazon Redshift distributes the table's rows to the compute nodes and slices according to the distribution style that you chose when you created the table. The distribution strategy that you choose for your database has important consequences for query performance, storage requirements, data loading, and maintenance. By choosing the best distribution style for each table, you can balance your data distribution and significantly improve overall system performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KEY distribution\n",
    "<p>The rows are distributed according to the values in one column. The leader node attempts to place matching values on the same node slice. If you distribute a pair of tables on the joining keys, the leader node collocates the rows on the slices according to the values in the joining columns so that matching values from the common columns are physically stored together.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVEN distribution\n",
    "<p>The leader node distributes the rows across the slices in a round-robin fashion(rows are always evenly distributed across slices), regardless of the values in any particular column. EVEN distribution is appropriate when a table does not participate in joins or when there is not a clear choice between KEY distribution and ALL distribution.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALL distribution\n",
    "<p>A copy of the entire table is distributed to every node. Where EVEN distribution or KEY distribution place only a portion of a table's rows on each node, ALL distribution ensures that every row is collocated for every join that the table participates in. &nbsp;ALL distribution multiplies the storage required by the number of nodes in the cluster, and so it takes much longer to load, update, or insert data into multiple tables. ALL distribution is appropriate only for relatively slow-moving tables; that is, tables that are not updated frequently or extensively. Small dimension tables do not benefit significantly from ALL distribution, because the cost of redistribution is low.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUTO distribution\n",
    "<p>With AUTO distribution, Amazon Redshift assigns an optimal distribution style based on the size of the table data. For example, Amazon Redshift initially assigns ALL distribution to a small table, then changes to EVEN distribution when the table grows larger. When a table is changed from ALL to EVEN distribution, storage utilization might change slightly. The change in distribution occurs in the background, in a few seconds. Amazon Redshift never changes the distribution style from EVEN to ALL. If you don't specify a distribution style with the CREATE TABLE statement, Amazon Redshift applies AUTO distribution.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choosing a sort key in Amazon Redshift  \n",
    "\n",
    "When you create a table, you can define one or more of its columns as sort keys. When data is initially loaded into the empty table, the rows are stored on disk in sorted order. Information about sort key columns is passed to the query planner, and the planner uses this information to construct plans that exploit the way that the data is sorted. You can specify either a compound or interleaved sort key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interleaved\n",
    "<p>An interleaved sort key gives equal weight to each column in the sort key, so query predicates can use any subset of the columns that make up the sort key, in any order. The performance improvements you gain by implementing an interleaved sort key should be weighed against increased load and vacuum times. Don’t use an interleaved sort key on columns with monotonically increasing attributes, such as identity columns, dates, or timestamps.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compound\n",
    "<p>A compound sort key is more efficient when query predicates use a prefix, which is a subset of the sort key columns in order.&nbsp;<br>Compound sort keys might speed up joins, GROUP BY and ORDER BY operations, and window functions that use PARTITION BY and ORDER BY. For example, a merge join, which is often faster than a hash join, is feasible when the data is distributed and presorted on the joining columns. Compound sort keys also help improve compression.<br>The performance benefits of compound sorting decrease when queries depend only on secondary sort columns, without referencing the primary columns. COMPOUND is the default sort type.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compression encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"fr-view\"><p>A compression encoding specifies the type of compression that is applied to a column of data values as rows are added to a table. Learn more about the different compression types: </p><ul><li><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_Raw_encoding.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">Raw Encoding:</a>Raw encoding is the default encoding for columns that are designated as sort keys and columns that are defined as BOOLEAN, REAL, or DOUBLE PRECISION data types. With raw encoding, data is stored in raw, uncompressed form. </li><li><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/az64-encoding.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">AZ64 Encoding:</a>AZ64 is Amazon's proprietary compression encoding algorithm designed to achieve a high compression ratio and improved query processing. At its core, the AZ64 algorithm compresses smaller groups of data values and uses single instruction, multiple data (SIMD) instructions for parallel processing. Use AZ64 to achieve significant storage savings and high performance for numeric, date, and time data types.</li><li><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_Byte_dictionary_encoding.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">Byte-Dictionary Encoding</a></li><li><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_Delta_encoding.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">Delta Encoding</a></li><li><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/lzo-encoding.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">LZO Encoding:</a> provides a very high compression ratio with good performance. LZO encoding works especially well for CHAR and VARCHAR columns that store very long character strings, especially free form text, such as product descriptions, user comments, or JSON strings.</li><li><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_MostlyN_encoding.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">Mostly Encoding</a></li><li><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_Runlength_encoding.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">Runlength Encoding</a></li><li><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_Text255_encoding.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">Text255 and Text32k Encodings</a></li><li><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/zstd-encoding.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">Zstandard Encoding</a></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"fr-view\"><p>If no compression is specified in a CREATE TABLE or ALTER TABLE statement, Amazon Redshift automatically assigns compression encoding as follows:</p><ul><li><p>Columns that are defined as BOOLEAN, REAL, or DOUBLE PRECISION data types are assigned RAW compression.</p></li><li><p>Columns that are defined as SMALLINT, INTEGER, BIGINT, DECIMAL, DATE, TIMESTAMP, or TIMESTAMPTZ data types are assigned AZ64 compression.</p></li><li><p>Columns that are defined as CHAR or VARCHAR data types are assigned LZO compression.</p></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizing size of data in Amazon Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"fr-view\"><p>Best practices for optimizing data size:&nbsp;</p><ul><li>Split your load data files so that the files are about equal size, between 1 MB and 1 GB after compression. For optimum parallelism, the ideal size is between 1 MB and 125 MB after compression.&nbsp;</li><li>The number of files should be a multiple of the number of slices in your cluster.</li><li>When loading data, we strongly recommend that you individually compress your load files using gzip, lzop, bzip2, or Zstandard when you have large datasets. See more Amazon Redshift loading best practices <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-compress-data-files.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\">here</a>.</li><li>You can also apply compression encodings when creating a table, as discussed previously in the module.</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing (24 %)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Visualization (18%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://aws.amazon.com/big-data/datalakes-and-analytics/\">Data Lakes and Analytics on AWS</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "\ttable.tableizer-table {\n",
    "\t\tfont-size: 12px;\n",
    "\t\tborder: 1px solid #CCC; \n",
    "\t\tfont-family: Arial, Helvetica, sans-serif;\n",
    "\t} \n",
    "\t.tableizer-table td {\n",
    "\t\tpadding: 4px;\n",
    "\t\tmargin: 3px;\n",
    "\t\tborder: 1px solid #CCC;\n",
    "\t}\n",
    "\t.tableizer-table th {\n",
    "\t\tbackground-color: #104E8B; \n",
    "\t\tcolor: #FFF;\n",
    "\t\tfont-weight: bold;\n",
    "\t}\n",
    "</style>\n",
    "<table class=\"tableizer-table\">\n",
    "<thead><tr class=\"tableizer-firstrow\"><th></th><th>Cost</th><th>Performance</th><th>Durability and availability</th><th>Scalability and elasticity</th><th>Interfaces</th></tr></thead><tbody>\n",
    " <tr><td>Amazon Athena</td><td>Pay for the resources you consume. Priced per query, per TB of data scanned, and charges based on the amount of data scanned by the query. </td><td>You can improve the performance of your query by compressing, partitioning, and converting your data into columnar formats.</td><td>Amazon Athena is highly available and executes queries using compute resources across multiple facilities, automatically routing queries appropriately if a particular facility is unreachable. </td><td>Athena is serverless, so there is no infrastructure to set up or manage, and it can scale automatically, as needed.</td><td>Athena Console and CLI, API via SDK and JDBC. Athena integrates with Amazon QuickSight. Athena that are registered with the AWS Glue Data Catalog.</td></tr>\n",
    " <tr><td>Amazon ES</td><td>Pay only for what you use. You are charged for Amazon ES instance hours, Amazon EBS storage (if you choose this option), and standard data transfer fees.</td><td>Performance depends on multiple factors including: •instance type •workload •index •number of shards used •read replica configuration •storage configuration (instance or EBS). Indexes are made up of shards of data which can be distributed on different instances in multiple Availability Zones.</td><td>Enable zone awareness for high availability. When Zone Awareness is enabled, Amazon ES distributes the instances supporting the domain across two different Availability Zones. Then, if you enable replicas in Amazon ES, the instances are automatically distributed to deliver cross-zone replication.  </td><td>You can add or remove instances, and modify Amazon EBS volumes to accommodate data growth.</td><td>Amazon ES supports integration with AWS services for streaming data. Some sources have built-in support for Amazon ES, others use Lambda functions as event handlers.</td></tr>\n",
    " <tr><td>Amazon EMR</td><td>you only pay for the hours the cluster is up. You can launch a persistent cluster that stays up indefinitely or a temporary cluster that terminates after the analysis is complete.</td><td>Amazon EMR performance is driven by the type and number of EC2 instances you choose to run your analytics. Consider processing requirements, sufficient memory, storage, and processing power.</td><td>By default, Amazon EMR is fault tolerant for core node failures and continues job execution if a slave node goes down. When a core node fails, Amazon EMR will provision a new node. If all nodes in the cluster are lost, Amazon EMR will not replace them. </td><td>You can resize your cluster to add instances for peak workloads and remove instances to control costs when peak workloads subside</td><td>Amazon EMR supports many tools on top of Hadoop that can be used for big data analytics and each has its own interfaces.</td></tr>\n",
    " <tr><td>Amazon Kinesis Data Stream</td><td>•There are just two pricing components, an hourly charge per shard and a charge for each 1 million PUT transactions. •The use of enhanced fan-out configuration or extended retention periods has additional costs.</td><td>Choose throughput capacity in terms of shards. The enhanced fan-out option can improve performance by increasing the throughput available to each individual consumer.</td><td>Kinesis Data Streams synchronously replicates data across three Availability Zones in an AWS Region, and stores that data for up to seven days.</td><td>The initial scale is based on the number of shards you select for the stream. You can increase or decrease the capacity of the stream at any time. Use API calls or development tools to automate scaling.</td><td>Producers: Amazon Kinesis PUT API, SDK or toolkit abstraction, the Amazon Kinesis Producer Library (KPL), or the Amazon Kinesis Agent. Consumer:KCL, Kinesis Data Analytics, Kinesis Data Firehose, and AWS Lambda.</td></tr>\n",
    " <tr><td>Amazon Kinesis Data Firehose</td><td>Volume of data ingested (the number of data records you send to the service) *( the size of each record rounded up to the nearest 5KB). format conversion like Parquet, ORC has charges</td><td>Specify a batch size or batch interval and data compression to control how quickly data is uploaded to destinations</td><td>All Kinesis services are fully managed.</td><td>Streams automatically scale up and down based on the data rate you specify for the stream.</td><td>Kinesis data stream, the Kinesis Agent, or the Kinesis Data Firehose API using the AWS SDK to write to a Kinesis Data Firehose stream. Amazon CloudWatch Logs, CloudWatch Events, or AWS IoT Core as your data source. Kinesis Data Firehose streams can deliver data to one of four destinations: Amazon S3, Amazon ES, Amazon Redshift, or Splunk.</td></tr>\n",
    " <tr><td>Amazon Data Analytics</td><td>Kinesis Processing Units (or KPUs) by hour used to run your stream processing application. 1 KPU is 1 vCPU compute and 4 GB memory. For Java applications, you are charged a single additional KPU per application for application orchestration</td><td>Amazon Kinesis Data Analytics elastically scales your application to accommodate the data throughput of your source stream and your query complexity for most scenarios. Kinesis Data Analytics provisions capacity in the form of Kinesis Processing Units (KPU).</td><td>For Kinesis Data Analytics applications, you can create and delete durable application backups through a simple API call.</td><td>Set up your application for your future scaling needs by proactively increasing the number of input in-application streams from the default (one). Use multiple streams and Kinesis Data Analytics for SQL applications if your application has scaling needs beyond 100 MB/second.                               Use Kinesis Data Analytics for Java Applications if you want to use a single stream and application.  </td><td>Kinesis data stream or a Kinesis Data Firehose delivery stream.external destination such as an Amazon Kinesis data stream, a Kinesis Data Firehose delivery stream, or an AWS Lambda function.</td></tr>\n",
    " <tr><td>Amazon Redshift</td><td>size and number of nodes of your cluster. No additional charge for backup storage up to 100% of your provisioned storage. Backup storage beyond the provisioned storage size, and backups stored after your cluster is terminated, are billed at standard Amazon S3 rates. No data transfer charge for communication between Amazon S3 and Amazon Redshift.</td><td>Columnar storage, data compression, and zone maps to reduce the amount of I/O needed to perform queries. 10 GigE mesh network to maximize throughput between nodes.</td><td>Amazon Redshift automatically detects and replaces a failed node in your data warehouse cluster. The data warehouse cluster is read-only until a replacement node is provisioned and added to the DB, which typically only takes a few minutes. </td><td>You can change the number or type of nodes in your data warehouse. You can use elastic resize to scale your cluster by changing the number of nodes. Or, you can use classic resize to scale the cluster by specifying a different node type</td><td>JDBC and ODBC. You can ingest data into your data warehouse cluster as well as to and from Amazon S3 and DynamoDB.You can load streaming data into Amazon Redshift using Amazon Kinesis Data Firehose.</td></tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "\ttable.tableizer-table {\n",
    "\t\tfont-size: 12px;\n",
    "\t\tborder: 1px solid #CCC; \n",
    "\t\tfont-family: Arial, Helvetica, sans-serif;\n",
    "\t} \n",
    "\t.tableizer-table td {\n",
    "\t\tpadding: 4px;\n",
    "\t\tmargin: 3px;\n",
    "\t\tborder: 1px solid #CCC;\n",
    "\t}\n",
    "\t.tableizer-table th {\n",
    "\t\tbackground-color: #104E8B; \n",
    "\t\tcolor: #FFF;\n",
    "\t\tfont-weight: bold;\n",
    "\t}\n",
    "</style>\n",
    "<table class=\"tableizer-table\">\n",
    "<thead><tr class=\"tableizer-firstrow\"><th></th><th>Ideal Patterns</th><th>Anti-Patterns</th></tr></thead><tbody>\n",
    " <tr><td>Amazon Athena</td><td><div class=\"fr-view\"><ol><li><strong>Interactive ad hoc querying for weblogs</strong> – Athena is a good tool for interactive <strong>one-time SQL queries</strong> against data on Amazon S3. For example, you could use Athena to run a query on web and application logs to troubleshoot a performance issue. Athena integrates with Amazon QuickSight for easy visualization.</li><li><strong>Interactive Analytical Solutions with notebook-based solutions</strong> - Data scientists and Analysts are often concerned about managing the infrastructure behind big data platforms while running notebook-based solutions such as RStudio, Jupyter, and Zeppelin. Amazon Athena makes it easy to analyze data using standard SQL without the need to manage infrastructure.&nbsp;</li><li><strong>Analyze AWS service logs</strong>&nbsp; – AWS CloudTrail, Amazon CloudFront, Elastic Load Balancing and Amazon VPC flow logs can be analyzed with Athena. The logs allow you to investigate network traffic patterns and identify threats and risks across your Amazon VPC estate.</li><li><strong>Query staging data before loading into Amazon Redshift</strong> – You can stage your raw data in Amazon S3 before processing and loading it into Amazon Redshift, and then use Athena to query that data.</li></ol></div></td><td><div class=\"fr-view\"><ol><li><strong>Enterprise Reporting and Business Intelligence Workloads</strong> – <strong>Amazon Redshift is a better tool&nbsp;</strong>for Enterprise Reporting and Business Intelligence Workloads involving iceberg queries or cached data at the nodes. Data warehouses pull data from many sources, format and organize it, store it, and support complex, high speed queries that produce business reports. The query engine in Amazon Redshift has been optimized to perform especially well on data warehouse workloads.</li><li><strong>ETL Workloads</strong> – You should use <strong>Amazon EMR/AWS Glue</strong> if you are looking for an ETL tool to process extremely large datasets and analyze them with the latest big data processing frameworks such as Spark, Hadoop, Presto, or Hbase.</li><li><strong>RDBMS</strong> – Athena is not a relational/transactional database. It is not meant to be a replacement for SQL engines like MySQL.</li></ol></div></td></tr>\n",
    " <tr><td>Amazon ES</td><td><div class=\"fr-view\"><ol><li><strong>Analyze activity logs</strong>, e.g., logs for customer facing applications or websites</li><li><strong>Analyze CloudWatch logs</strong></li><li><strong>Analyze product usage data</strong> coming from various services and systems</li><li><strong>Analyze social media sentiments</strong>, CRM data and find trends for your brand and products</li><li><strong>Analyze data stream updates from other AWS service</strong>s, e.g., Amazon Kinesis Data Streams and Amazon DynamoDB</li><li><strong>Provide customers a rich search</strong> and navigation experience.</li><li><strong>Usage monitoring</strong> for mobile applications</li></ol></div></td><td><div class=\"fr-view\"><ol><li><strong>Online transaction processing (OLTP)</strong> - Amazon ES is a real-time distributed search and analytics engine. There is no support for transactions or processing on data manipulation. If your requirement is for a fast transactional system, then a relational database system built on <strong>Amazon RDS</strong>, or a <strong>non-relational database</strong> offering functionality such as <strong>DynamoDB</strong>, is a better choice.</li><li><strong>Ad hoc data querying&nbsp;</strong>– While Amazon ES takes care of the operational overhead of building a highly scalable Elasticsearch cluster if running Ad hoc queries or one-off queries against your data set is your use-case, <strong>Amazon Athena is a better choice</strong>.</li></ol></div></td></tr>\n",
    " <tr><td>Amazon EMR</td><td><div class=\"fr-view\"><p>Amazon EMR’s flexible framework <strong>reduces large processing problems</strong> and data sets into <strong>smaller jobs</strong> and <strong>distributes them&nbsp;</strong>across many compute nodes in a Hadoop cluster. This capability lends itself to many usage patterns with big data analytics including:</p><ol><li><strong>Log processing and analytics</strong></li><li><strong>Large extract, transform, and load</strong> (ETL) data movement</li><li><strong>Risk modeling</strong> and <strong>threat analytics</strong></li><li><strong>Ad targeting</strong> and <strong>clickstream analytics</strong></li><li><strong>Genomics</strong></li><li><strong>Predictive analytics</strong></li><li><strong>Ad hoc data mining</strong> and analytics</li></ol></div></td><td><div class=\"fr-view\"><ol><li><strong>Small data sets&nbsp;</strong>– Amazon EMR is <strong>built for massively parallel processing</strong>; if your data set is small enough to run quickly on a single machine, in a single thread, the added overhead to map and reduce jobs may not be worth it for small data sets that can easily be processed in memory on a single system.</li><li><strong>ACID transaction requirements</strong> – While there are ways to achieve ACID (atomicity, consistency, isolation, durability) or limited ACID on Hadoop, using another database, such as Amazon RDS or a relational database running on Amazon EC2 may be a better option for workloads with stringent requirements.</li></ol></div></td></tr>\n",
    " <tr><td>Amazon Kinesis</td><td><div class=\"fr-view\"><ol><li><strong>Real-time data analytics</strong> – Kinesis Data Streams enables real-time data analytics on streaming data, such as analyzing <strong>website clickstream data</strong> and <strong>customer engagement</strong> analytics.</li><li><strong>Log and data feed intake and processing</strong> – With Kinesis Data Streams, you can have producers <strong>push data directly into an Amazon Kinesis data stream</strong>. For example, you can submit system and application logs to Kinesis Data Streams and access the stream for processing within seconds. This <strong>prevents the log data from being lost if the front-end or application server fails</strong>, and <strong>reduces local log storage</strong> on the source. Kinesis Data Streams provides <strong>accelerated data intake&nbsp;</strong>because you are not batching up the data on the servers before you submit it for intake.</li><li><strong>Real-time metrics and reporting</strong> – You can use data ingested into Kinesis Data Streams for extracting metrics and generating KPIs to power reports and dashboards at real-time speeds. This enables data-processing application logic to work on data as it is streaming in continuously, rather than wait for data batches to arrive.</li></ol></div></td><td><div class=\"fr-view\"><ol><li><strong>Small scale consistent throughput&nbsp;</strong>– Even though Kinesis Data Streams works for streaming data at 200 KB/sec or less, it is designed and optimized for<strong>&nbsp;larger data throughputs</strong>.</li><li><strong>Long-term data storage and analytics</strong> –Kinesis Data Streams is not suited for long-term data storage. By default, data is retained for 24 hours, and you can extend the retention period by up to 7 days. You can move any data that needs to be stored for longer than 7 days into another durable storage service such as Amazon S3, Amazon S3 Glacier, Amazon Redshift, or DynamoDB.</li></ol></div></td></tr>\n",
    " <tr><td>Amazon Redshift</td><td><div class=\"fr-view\"><p>Amazon Redshift is ideal for<strong> online analytical processing (OLAP)</strong> using your existing business intelligence tools. Organizations are using Amazon Redshift to:</p><ol><li><strong>Analyze global sales data</strong> for multiple products</li><li>Store <strong>historical stock trade data</strong></li><li><strong>Analyze ad impressions</strong> and clicks</li><li><strong>Aggregate gaming data</strong></li><li><strong>Analyze social trends</strong></li><li><strong>Measure clinical quality</strong>, <strong>operation efficiency</strong>, and <strong>financial performance</strong> in health care</li></ol></div></td><td><div class=\"fr-view\"><ol><li><strong>Small data sets&nbsp;</strong>– Amazon Redshift is <strong>built for parallel processing across a cluster</strong>. If your data set is <strong>less than a hundred gigabytes</strong>, you are <strong>not going to get all the benefits</strong> that Amazon Redshift has to offer and <strong>Amazon RDS may be a better solution</strong>.</li><li><strong>On-line transaction processing (OLTP)</strong> – Amazon Redshift is<strong>&nbsp;designed for data warehouse workloads&nbsp;</strong>producing <strong>extremely fast and inexpensive analytic capabilities</strong>. If you require a fast transactional system, you may want to choose a traditional relational database system built on <strong>Amazon RDS</strong> or a Non-relational database offering, such as <strong>DynamoDB</strong>.</li></ol></div></td></tr>\n",
    " <tr><td>Amazon QuickSight</td><td><div class=\"fr-view\"><p>Amazon QuickSight is an ideal Business Intelligence tool allowing end-users to create visualizations that provide insight into their data to help them make better business decisions. Use cases include:</p><ol><li>Quick <strong>interactive ad-hoc exploration&nbsp;</strong>and <strong>optimized visualization</strong> of data</li><li>Create and share <strong>dashboards and KPI’s</strong> to provide insight into your data</li><li>Create<strong>&nbsp;Stories&nbsp;</strong>which are <strong>guided tours through specific views of an analysis</strong> and allow you to share insights and collaborate with others. They are used to convey key points, a thought process, or the evolution of analysis for collaboration.</li><li><strong>Analyze and&nbsp;</strong><strong>visualize data coming from logs and stored in Amazon S3</strong></li><li><strong>Analyze and visualize data from on-premises databases</strong> like SQL Server, Oracle, PostgreSQL, and MySQL</li><li><strong>Analyze and visualize data in various AWS resources</strong>, e.g., Amazon RDS databases, Amazon Redshift, Amazon Athena, and Amazon S3.</li><li><strong>Analyze and visualize data in software as a service (SaaS) applications&nbsp;</strong>like Salesforce.</li><li><strong>Analyze and visualize data in data sources</strong> that can be connected to <strong>using JDBC/ODBC</strong> connection<strong>.</strong></li></ol></div></td><td><div class=\"fr-view\"><ol><li><strong>Highly formatted canned Reports</strong> – Amazon QuickSight is much more suited for ad hoc query, analysis and visualization of data. For highly formatted reports e.g. formatted financial statements consider using a different tool.</li><li><strong>ETL</strong> - While Amazon QuickSight can perform some transformations <strong>it is not a full-fledged ETL tool</strong>. AWS offers <strong>AWS Glue</strong>, which is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics.</li></ol></div></td></tr>\n",
    " <tr><td>Amazon SageMaker</td><td><div class=\"fr-view\"><ol><li><strong>Enable applications to flag suspicious transactions</strong> – Build an ML model that predicts whether a new transaction is legitimate or fraudulent.</li><li><strong>Forecast product demand</strong> – Input historical order information to predict future order quantities.</li><li><strong>Personalize application content</strong> – Predict which items a user will be most interested in, and retrieve these predictions from your application in real-time.</li><li><strong>Predict user activity&nbsp;</strong>– Analyze user behavior to customize your website and provide a better user experience.</li><li><strong>Listen to social media</strong> – Ingest and analyze social media feeds that potentially impact business decisions.</li></ol></div></td><td><div class=\"fr-view\"><ol><li><strong>Very large data sets</strong> – Terabyte-scale ingestion of data is not currently supported. Using Amazon EMR to run Spark’s Machine Learning Library (MLlib) is a common tool for such a use case.</li><li>Cases, where you need full control over your ML environment,&nbsp;are not ideal for Amazon SageMaker since it is a managed service.</li></ol></div></td></tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"src/AWS_Analytics_Services.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon QuickSight visual types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cost\n",
    "Amazon QuickSight has two different editions for pricing; standard edition and enterprise edition. Pricing is based on an annual subscription. Both standard and enterprise editions include SPICE (Super-fast, Parallel, and In-memory Calculation Engine) capacity, and you can get additional SPICE capacity for a monthly add on cost. Month to month billing options are available for both the editions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance\n",
    "Amazon QuickSight is built with SPICE. Built from the ground up for the cloud, SPICE uses a combination of columnar storage, in-memory technologies enabled through the latest hardware innovations, and machine code generation to run interactive queries on large datasets and get rapid responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Durability and availability\n",
    "SPICE automatically replicates data for high availability and enables Amazon QuickSight to scale to hundreds of thousands of users who can all simultaneously perform fast interactive analysis across a wide variety of AWS data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scalability and elasticity\n",
    "QuickSight is a fully managed service that internally takes care of scaling to meet the demands of end-users. You can seamlessly grow your data from a few hundred megabytes to many terabytes of data without managing any infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interfaces\n",
    "Amazon QuickSight can connect to a wide variety of data sources including flat files (CSV, TSV, CLF, ELF),  on-premises databases like SQL Server, MySQL, and PostgreSQL, and AWS data sources including Amazon RDS, Amazon Aurora, Amazon Redshift, Amazon Athena and Amazon S3, and SaaS applications like Salesforce. You can also export analyzes from a visual to a file with CSV format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%;\"><thead><tr><th style=\"width:26.3698%;background-color:rgb(13, 78, 70);\"><span style=\"color:rgb(255, 255, 255);font-weight:bold;font-size:16px;\">Type</span></th><th style=\"width:73.4989%;background-color:rgb(13, 78, 70);\"><span style=\"color:rgb(255, 255, 255);font-weight:bold;font-size:16px;\">Use</span><br></th></tr></thead><tbody><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/autograph.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">AutoGraph<br></span></a></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">AutoGraph isn't a visual type itself but instead lets you tell Amazon QuickSight to choose the visual type for you.</span></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/bar-charts.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">Bar charts</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use a horizontal or vertical bar chart to create single-measure, multi-measure, or clustered bar charts. Use any of the stacked bar chart visual types to create stacked bar charts.</span></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/combo-charts.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">Combo charts</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use the combo chart visual types to create a single visualization that shows two different types of data. &nbsp;These are also known as line and column charts. Amazon QuickSight supports clustered bar combo and stacked bar combo charts.</span><br></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/donut-chart.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">Donut charts</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use donut charts to compare values for items in a dimension. The best use for this type of chart is to show a percentage of the total amount. &nbsp;</span><br></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/gauge-chart.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">Gauge charts</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use gauge charts to compare values for items in a measure. A gauge chart is similar to a non-digital gauge, for example, a gas gauge in an automobile. It displays how much there is of the thing you are measuring.</span><br></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/geospatial-charts.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">Geospatial charts (maps)</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use geospatial charts to show differences in data values across a geographical map. The map allows you to zoom in and out.</span><br></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/heat-map.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">Heat maps</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use heat maps to show a measure for the intersection of two dimensions, with color-coding to easily differentiate where values fall in the range.</span><br></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/kpi.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">KPIs</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use a KPI to visualize a comparison between a key-value and its target value.</span><br></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/line-charts.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">Line charts</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use line charts to compare changes in measure values over a period of time.</span><br></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/pie-chart.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">Pie charts</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use pie charts to compare values for items in a dimension. The best use for this type of chart is to show a percentage of the total amount. &nbsp; &nbsp; &nbsp;</span><br></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/pivot-table.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">Pivot tables</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use pivot tables to show measure values for the intersection of two dimensions. Heat maps and pivot tables display data in a similar tabular fashion. Use a heat map if you want to identify trends and outliers because the use of color makes these easier to spot. Use a pivot table if you want to analyze data on the visual. &nbsp; &nbsp;</span><br></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/scatter-plot.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">Scatter plots</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use scatter plots to visualize two or three measures for a dimension.</span><br></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/tabular.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">Tables</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use a table visual to see a customized table view of your data. To create a table visual, choose at least one field of any data type.</span><br></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/tree-map.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">Tree maps</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use tree maps to visualize one or two measures for a dimension. Each rectangle on the tree represents one item in the dimension.</span><br></td></tr><tr><td style=\"text-align:center;width:26.3698%;\"><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/word-cloud.html\" rel=\"noopener noreferrer\" target=\"_blank\" tabindex=\"0\" aria-hidden=\"false\"><span style=\"font-size:16px;\">Word clouds</span></a><br></td><td style=\"text-align:left;width:73.4989%;\"><span style=\"font-size:16px;\">Use word clouds as an engaging way to display how often a word is used in relation to other words in a data set.</span><br></td></tr></tbody></table>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
